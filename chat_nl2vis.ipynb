{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ulYoEhykeZlB"
      },
      "source": [
        "## Load dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-N_BcBb9gXV"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import json\n",
        "import files\n",
        "#openai.api_key = \"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\"\n",
        "#openai.api_key = \"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\"\n",
        "openai.api_key = \"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\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CGFw0z-3ehhH"
      },
      "source": [
        "## Basic Use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_Wz6xfTHw-H",
        "outputId": "a8955e11-71f2-4484-c463-957a42eaee14"
      },
      "outputs": [],
      "source": [
        "### Basic Use\n",
        "completion = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"user\", \"content\": \"Tell the world about the ChatGPT API in the style of a pirate.\"}\n",
        "  ]\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2l8b-xa-KpC"
      },
      "outputs": [],
      "source": [
        "### A chatBot wrapper\n",
        "class ChatBot:\n",
        "    def __init__(self, system=\"\"):\n",
        "        self.system = system\n",
        "        self.messages = []\n",
        "        if self.system:\n",
        "            self.messages.append({\"role\": \"system\", \"content\": system})\n",
        "    \n",
        "    def __call__(self, message):\n",
        "        self.messages.append({\"role\": \"user\", \"content\": message})\n",
        "        result = self.execute()\n",
        "        self.messages.append({\"role\": \"assistant\", \"content\": result})\n",
        "        return result\n",
        "    \n",
        "    def execute(self):\n",
        "        completion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=self.messages)\n",
        "        # Uncomment this to print out token usage each time, e.g.\n",
        "        # {\"completion_tokens\": 86, \"prompt_tokens\": 26, \"total_tokens\": 112}\n",
        "        print(completion.usage)\n",
        "        return completion.choices[0].message.content\n",
        "\n",
        "simon = ChatBot(\"You are a chatbot imitating Simon Willison. Pretend to be Simon.\")\n",
        "print(simon.execute())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMo0cAgRSrU-",
        "outputId": "8d0004ae-d40e-4ee9-9a27-f7dbe5e0fda2"
      },
      "outputs": [],
      "source": [
        "### User role\n",
        "content = \"Who was the first man on the moon?\"\n",
        "messages=[\n",
        "    {\"role\": \"user\", \"content\": content}\n",
        "]\n",
        "completion = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=messages\n",
        ")\n",
        "\n",
        "chat_response = completion.choices[0].message.content\n",
        "print(f'ChatGPT: {chat_response}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZY_Zr0pQxql",
        "outputId": "fe2c0e9b-18bc-4c73-c664-18cd157499e3"
      },
      "outputs": [],
      "source": [
        "### System role to set the behavior of the assistant\n",
        "\n",
        "messages = [\n",
        " {\"role\": \"system\", \"content\" : \"Youâ€™re a kind helpful assistant\"}\n",
        "]\n",
        "content = \"Who was the first man on the moon?\"\n",
        "messages.append({\"role\": \"user\", \"content\": content})\n",
        "\n",
        "completion = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=messages\n",
        ")\n",
        "\n",
        "chat_response = completion.choices[0].message.content\n",
        "print(f'ChatGPT: {chat_response}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8yQitxIRr50"
      },
      "outputs": [],
      "source": [
        "### Assistant role to store prior responses\n",
        "### conduct a conversation\n",
        "messages=[]\n",
        "while True:\n",
        "    content = input(\"User: \")\n",
        "    messages.append({\"role\": \"user\", \"content\": content})\n",
        "    \n",
        "    completion = openai.ChatCompletion.create(\n",
        "      model=\"gpt-3.5-turbo\",\n",
        "      messages=messages\n",
        "    )\n",
        "\n",
        "    chat_response = completion.choices[0].message.content\n",
        "    print(f'ChatGPT: {chat_response}')\n",
        "    messages.append({\"role\": \"assistant\", \"content\": chat_response})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8KMoOt7Wxts",
        "outputId": "1f2da208-9051-4e6c-8191-69dd2380c089"
      },
      "outputs": [],
      "source": [
        "### Text Completion\n",
        "prompt = \"\"\"\n",
        "Decide whether a Tweet's sentiment is positive, neutral, or negative.\n",
        "\n",
        "Tweet: I didn't like the new Batman movie!\n",
        "Sentiment:\n",
        "\"\"\"\n",
        "\n",
        "response = openai.Completion.create(\n",
        "              model=\"text-davinci-003\", #\"text-davinci-002\" \"davinci\"\n",
        "              prompt=prompt,\n",
        "              max_tokens=100,\n",
        "              temperature=0\n",
        "            )\n",
        "\n",
        "print(response.choices[0].text)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OVStf6r_A9bg"
      },
      "source": [
        "# NL2Vis"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3hVjOMXPKZRK"
      },
      "source": [
        "## basic examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Di19VZQBAlW",
        "outputId": "82715bb2-00df-486b-fe3a-c84c7b6129eb"
      },
      "outputs": [],
      "source": [
        "### data synthesis part1\n",
        "\n",
        "instruction=\"You are a data synthesizer to generate data from natural langugae to visualization code based on the following iteractions and database schema.\"\n",
        "information = \"interactions: | What are all the airlines? | SELECT * FROM AIRLINES | Of these, which is Jetblue Airways? | SELECT * FROM AIRLINES WHERE Airline  =  \\\"JetBlue Airways\\\" | What is the country corresponding it? | SELECT Country FROM AIRLINES WHERE Airline  =  \\\"JetBlue Airways\\\" serialized_schema: | flight_2 | airlines : uid , airline , abbreviation , country | airports : city , airportcode , airportname , country , countryabbrev | flights : airline , flightno , sourceairport , destairport\"\n",
        "messages = [\n",
        " {\"role\": \"system\", \"content\" : instruction},\n",
        " {\"role\": \"system\", \"content\" : information}\n",
        "]\n",
        "content=\"Please first create some data for the given database schema, the queries should be taken into account to make related data for visualization\"\n",
        "messages.append({\"role\": \"user\", \"content\": content})\n",
        "completion = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=messages\n",
        ")\n",
        "chat_response = completion.choices[0].message.content\n",
        "messages.append({\"role\": \"assistant\", \"content\": chat_response}) \n",
        "print(chat_response)\n",
        "\n",
        "content=\"Please use the data to create corresponding vega-lite visualizations based on the given interactions of natural language questions and SQL queries\"\n",
        "messages.append({\"role\": \"user\", \"content\": content}) \n",
        "completion = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=messages\n",
        ")\n",
        "chat_response = completion.choices[0].message.content\n",
        "messages.append({\"role\": \"assistant\", \"content\": chat_response}) \n",
        "print(chat_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMK7oEPtj_Op",
        "outputId": "b28bdb9d-efd2-4d8b-f6ef-fef729a56ab8"
      },
      "outputs": [],
      "source": [
        "### data synthesis part2\n",
        "\n",
        "instruction=\"You are a data synthesizer to generate data from natural langugae to visualization code based on the following database schema.\"\n",
        "information = \"serialized_schema: | flight_2 | airlines : uid , airline , abbreviation , country | airports : city , airportcode , airportname , country , countryabbrev | flights : airline , flightno , sourceairport , destairport\"\n",
        "messages = [\n",
        " {\"role\": \"system\", \"content\" : instruction},\n",
        " {\"role\": \"system\", \"content\" : information}\n",
        "]\n",
        "content=\"Please first create some data for the given database schema to be used to make visualizations later.\"\n",
        "messages.append({\"role\": \"user\", \"content\": content})\n",
        "completion = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=messages\n",
        ")\n",
        "chat_response = completion.choices[0].message.content\n",
        "messages.append({\"role\": \"assistant\", \"content\": chat_response}) \n",
        "print(chat_response)\n",
        "\n",
        "content=\"Please use the data to create some natural language questions and corresponding vega-lite visualizations. Try to follow a conversational manner and form a logically coherent interaction\"\n",
        "messages.append({\"role\": \"user\", \"content\": content}) \n",
        "completion = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=messages\n",
        ")\n",
        "chat_response = completion.choices[0].message.content\n",
        "messages.append({\"role\": \"assistant\", \"content\": chat_response}) \n",
        "print(chat_response)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate by step example"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "d7xzN8UuMvFV"
      },
      "source": [
        "### part 1 generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZR9LvRoNEfj"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import json\n",
        "import files\n",
        "with open('serialized_str.json', 'r') as file:\n",
        "    interactions = json.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxGEeNdXMxuj",
        "outputId": "1c3a48e2-9588-430c-d02a-7bdb5ef38ccf"
      },
      "outputs": [],
      "source": [
        "instruction=\"You are a data synthesizer to generate data from natural langugae to visualization code based on the following iteractions and database schema.\"\n",
        "#information = \"interactions: | What are all the airlines? | SELECT * FROM AIRLINES | Of these, which is Jetblue Airways? | SELECT * FROM AIRLINES WHERE Airline  =  \\\"JetBlue Airways\\\" | What is the country corresponding it? | SELECT Country FROM AIRLINES WHERE Airline  =  \\\"JetBlue Airways\\\" serialized_schema: | flight_2 | airlines : uid , airline , abbreviation , country | airports : city , airportcode , airportname , country , countryabbrev | flights : airline , flightno , sourceairport , destairport\"\n",
        "information = interactions[0]\n",
        "messages = [\n",
        " {\"role\": \"system\", \"content\" : instruction},\n",
        " {\"role\": \"system\", \"content\" : information}\n",
        "]\n",
        "content=\"Please first create some data for the given database schema, the queries should be taken into account to make related data for visualization\"\n",
        "messages.append({\"role\": \"user\", \"content\": content})\n",
        "completion = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=messages\n",
        ")\n",
        "chat_response = completion.choices[0].message.content\n",
        "messages.append({\"role\": \"assistant\", \"content\": chat_response}) \n",
        "print(chat_response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyQ6RPGhM4EL",
        "outputId": "93698c74-258d-403e-d49e-40e2cad046e0"
      },
      "outputs": [],
      "source": [
        "content=\"Please use the data to create corresponding vega-lite visualizations based on the given interactions of natural language questions and SQL queries\"\n",
        "messages.append({\"role\": \"user\", \"content\": content}) \n",
        "completion = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=messages\n",
        ")\n",
        "chat_response = completion.choices[0].message.content\n",
        "messages.append({\"role\": \"assistant\", \"content\": chat_response}) \n",
        "print(chat_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### nl2sql-nl2vis visualization from sparc\n",
        "\n",
        "import openai\n",
        "import json\n",
        "import re\n",
        "openai.api_key = \"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\"\n",
        "with open('serialized_str.json', 'r') as file:\n",
        "    interactions = json.load(file)\n",
        "\n",
        "instruction=\"You are a data synthesizer to generate data from natural langugae to visualization code based on the following iteractions and database schema.\"\n",
        "prompt1=\"Please first create some data for the given database schema, the queries should be taken into account to make related data for visualization.\"\n",
        "prompt2=\"Please use the data to create corresponding vega-lite visualizations based on the given interactions of natural language questions and SQL queries.\"\n",
        "prompt3=\"Ouput the data in a json format as a list of dictionaries which have two features: natural language description and the visualization code. The visualization code should include actual data. Please give ONLY the json format without any text explanations.\"\n",
        "results,predictions = [],[]\n",
        "pattern = r\"```(.*?)```\"\n",
        "for interaction in interactions[0:2]: \n",
        "    messages = [{\"role\": \"system\", \"content\": instruction},{\"role\": \"system\", \"content\": interaction},{\"role\": \"user\", \"content\": prompt1}] \n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        temperature=0,\n",
        "        max_tokens=2000,\n",
        "    )\n",
        "    chat_response= completion.choices[0].message.content\n",
        "    messages.append({\"role\": \"assistant\", \"content\": chat_response}) \n",
        "    messages.append({\"role\": \"user\", \"content\": prompt2})  \n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        temperature=0,\n",
        "        max_tokens=2000,\n",
        "    )\n",
        "    chat_response= completion.choices[0].message.content\n",
        "\n",
        "    messages.append({\"role\": \"assistant\", \"content\": chat_response}) \n",
        "    messages.append({\"role\": \"user\", \"content\": prompt3}) \n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        temperature=0,\n",
        "        max_tokens=2000,\n",
        "    )\n",
        "    \n",
        "    chat_response= completion.choices[0].message.content\n",
        "    code_matches = re.findall(pattern, chat_response, re.DOTALL)\n",
        "    result=code_matches[0].strip().replace(\"json\", \"\")\n",
        "    lst=eval(result)\n",
        "    results.append({\"input\": interaction, \"output\": lst})\n",
        "    predictions.append(lst)\n",
        "print(chat_response)\n",
        "with open(\"result_vis_interactions.json\", \"w\") as file:\n",
        "    json.dump(results, file, indent=4)\n",
        "with open(\"predictions_vis_interactions.json\", \"w\") as file:\n",
        "    json.dump(predictions, file, indent=4)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "amY2Pf3SElKj"
      },
      "source": [
        "### part 2 generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eoSlnpVMEnmQ"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import json\n",
        "import files\n",
        "with open('schema_str.json', 'r') as file:\n",
        "    schema = json.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXvVETGLEz_p",
        "outputId": "22620ea9-8ebb-4e56-c7af-e290a892d488"
      },
      "outputs": [],
      "source": [
        "instruction=\"You are a data synthesizer to generate data from natural langugae to visualization code based on the following database schema.\"\n",
        "#information = \"serialized_schema: | flight_2 | airlines : uid , airline , abbreviation , country | airports : city , airportcode , airportname , country , countryabbrev | flights : airline , flightno , sourceairport , destairport\"\n",
        "information = schema[0]\n",
        "print(information)\n",
        "messages = [\n",
        " {\"role\": \"system\", \"content\" : instruction},\n",
        " {\"role\": \"system\", \"content\" : information}\n",
        "]\n",
        "content=\"Please first create some data for the given database schema\"\n",
        "messages.append({\"role\": \"user\", \"content\": content})\n",
        "completion = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=messages\n",
        ")\n",
        "chat_response = completion.choices[0].message.content\n",
        "messages.append({\"role\": \"assistant\", \"content\": chat_response}) \n",
        "print(chat_response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "content=\"Please use the data to create some natural language questions and corresponding vega-lite visualizations. Try to follow a conversational manner and form a logically coherent interaction\"\n",
        "messages.append({\"role\": \"user\", \"content\": content}) \n",
        "completion = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=messages\n",
        ")\n",
        "chat_response = completion.choices[0].message.content\n",
        "messages.append({\"role\": \"assistant\", \"content\": chat_response}) \n",
        "print(chat_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### nl2vis visualization generation from schema\n",
        "\n",
        "import openai\n",
        "import json\n",
        "import re\n",
        "openai.api_key = \"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\"\n",
        "with open('schema_str.json', 'r') as file:\n",
        "    schemas = json.load(file)\n",
        "\n",
        "instruction=\"You are a data synthesizer to generate data from natural langugae to visualization code based on the following database schema.\"\n",
        "prompt1=\"Please first create some data for the given database schema to be used to make visualizations later.\"\n",
        "prompt2=\"Please use the data to create some natural language questions and corresponding vega-lite visualizations. Try to follow a conversational manner and form a logically coherent interaction.\"\n",
        "prompt3=\"Ouput the data in a json format as a list of dictionaries which have two features: natural language description and the visualization code. The visualization code should include actual data. Please give ONLY the json format without any text explanations\"\n",
        "predictions =[]\n",
        "pattern = r\"```(.*?)```\"\n",
        "\n",
        "for schema in schemas[0:2]: \n",
        "    messages = [{\"role\": \"system\", \"content\": instruction},{\"role\": \"system\", \"content\": schema},{\"role\": \"user\", \"content\": prompt1}] \n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        temperature=0,\n",
        "        max_tokens=2000,\n",
        "    )\n",
        "    chat_response= completion.choices[0].message.content\n",
        "    messages.append({\"role\": \"assistant\", \"content\": chat_response}) \n",
        "    messages.append({\"role\": \"user\", \"content\": prompt2})  \n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        temperature=0,\n",
        "        max_tokens=2000,\n",
        "    )\n",
        "    chat_response= completion.choices[0].message.content\n",
        "\n",
        "    messages.append({\"role\": \"assistant\", \"content\": chat_response}) \n",
        "    messages.append({\"role\": \"user\", \"content\": prompt3}) \n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        temperature=0,\n",
        "        max_tokens=2000,\n",
        "    )\n",
        "    chat_response= completion.choices[0].message.content  \n",
        "    code_matches = re.findall(pattern, chat_response, re.DOTALL)\n",
        "    result=code_matches[0].strip().replace(\"json\", \"\")\n",
        "    lst=eval(result)\n",
        "    predictions.append(lst)\n",
        "print(result)\n",
        "with open(\"predictions_vis_schema.json\", \"w\") as file:\n",
        "    json.dump(predictions, file, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "#pattern = r\"```json(.*?)```\"\n",
        "pattern = r\"```(.*?)```\"\n",
        "code_matches = re.findall(pattern, chat_response, re.DOTALL)\n",
        "code=code_matches[0].strip().replace(\"json\", \"\")\n",
        "print(code)\n",
        "# for code in code_matches:\n",
        "#     print(code.strip().replace(\"json\", \"\"))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate dataset"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### part 1: interactions used +example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\n",
            "    {\n",
            "        \"natural_language_query\": \"What are all the airlines?\",\n",
            "        \"visualization_code\": {\n",
            "            \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",\n",
            "            \"data\": {\n",
            "                \"values\": [\n",
            "                    {\"Airline\": \"JetBlue Airways\", \"Country\": \"United States\"},\n",
            "                    {\"Airline\": \"Delta Air Lines\", \"Country\": \"United States\"},\n",
            "                    {\"Airline\": \"United Airlines\", \"Country\": \"United States\"},\n",
            "                    {\"Airline\": \"American Airlines\", \"Country\": \"United States\"}\n",
            "                ]\n",
            "            },\n",
            "            \"mark\": \"bar\",\n",
            "            \"encoding\": {\n",
            "                \"x\": {\"field\": \"Airline\", \"type\": \"nominal\"},\n",
            "                \"y\": {\"aggregate\": \"count\", \"type\": \"quantitative\"}\n",
            "            }\n",
            "        }\n",
            "    },\n",
            "    {\n",
            "        \"natural_language_query\": \"Of these, which is Jetblue Airways?\",\n",
            "        \"visualization_code\": {\n",
            "            \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",\n",
            "            \"data\": {\n",
            "                \"values\": [\n",
            "                    {\"Airline\": \"JetBlue Airways\", \"Country\": \"United States\"}\n",
            "                ]\n",
            "            },\n",
            "            \"mark\": \"bar\",\n",
            "            \"encoding\": {\n",
            "                \"x\": {\"field\": \"Airline\", \"type\": \"nominal\"},\n",
            "                \"y\": {\"aggregate\": \"count\", \"type\": \"quantitative\"}\n",
            "            },\n",
            "            \"transform\": [\n",
            "                {\"filter\": {\"field\": \"Airline\", \"equal\": \"JetBlue Airways\"}}\n",
            "            ]\n",
            "        }\n",
            "    },\n",
            "    {\n",
            "        \"natural_language_query\": \"What is the country corresponding it?\",\n",
            "        \"visualization_code\": {\n",
            "            \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",\n",
            "            \"data\": {\n",
            "                \"values\": [\n",
            "                    {\"Country\": \"United States\"}\n",
            "                ]\n",
            "            },\n",
            "            \"mark\": \"bar\",\n",
            "            \"encoding\": {\n",
            "                \"x\": {\"field\": \"Country\", \"type\": \"nominal\"},\n",
            "                \"y\": {\"aggregate\": \"count\", \"type\": \"quantitative\"}\n",
            "            },\n",
            "            \"transform\": [\n",
            "                {\"filter\": {\"field\": \"Airline\", \"equal\": \"JetBlue Airways\"}},\n",
            "                {\"filter\": {\"field\": \"Country\", \"equal\": \"United States\"}}\n",
            "            ]\n",
            "        }\n",
            "    }\n",
            "]\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "### nl2sql-nl2vis visualization from sparc\n",
        "\n",
        "import openai\n",
        "import json\n",
        "openai.api_key = \"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\"\n",
        "with open('serialized_str.json', 'r') as file:\n",
        "    interactions = json.load(file)\n",
        "\n",
        "instruction=\"You are a data synthesizer to generate data from natural langugae to visualization code based on the following iteractions and database schema. Please follow these steps: 1. First create corresponding vega-lite visualizations based on the given interactions of natural language questions and SQL queries. 2. Ouput the data in a json format as a list of dictionaries which have two features: natural language query and the visualization code. The visualization code should include actual data. Please give ONLY the json format without any text explanations.\"\n",
        "example = \"Here is an example including input and output. INPUT: interactions: | What are all the airlines? | SELECT * FROM AIRLINES | Of these, which is Jetblue Airways? | SELECT * FROM AIRLINES WHERE Airline  =  \\\"JetBlue Airways\\\" | What is the country corresponding it? | SELECT Country FROM AIRLINES WHERE Airline  =  \\\"JetBlue Airways\\\" serialized_schema: | flight_2 | airlines : uid , airline , abbreviation , country | airports : city , airportcode , airportname , country , countryabbrev | flights : airline , flightno , sourceairport , destairport| OUTPUT: [\\n    {\\n        \\\"natural_language_query\\\": \\\"What are all the airlines?\\\",\\n        \\\"visualization_code\\\": {\\n            \\\"$schema\\\": \\\"https://vega.github.io/schema/vega-lite/v5.json\\\",\\n            \\\"data\\\": {\\n                \\\"values\\\": [\\n                    {\\\"Airline\\\": \\\"JetBlue Airways\\\", \\\"Country\\\": \\\"United States\\\"},\\n                    {\\\"Airline\\\": \\\"Delta Air Lines\\\", \\\"Country\\\": \\\"United States\\\"},\\n                    {\\\"Airline\\\": \\\"American Airlines\\\", \\\"Country\\\": \\\"United States\\\"},\\n                    {\\\"Airline\\\": \\\"United Airlines\\\", \\\"Country\\\": \\\"United States\\\"}\\n                ]\\n            },\\n            \\\"mark\\\": \\\"bar\\\",\\n            \\\"encoding\\\": {\\n                \\\"x\\\": {\\\"field\\\": \\\"Airline\\\", \\\"type\\\": \\\"nominal\\\"},\\n                \\\"y\\\": {\\\"aggregate\\\": \\\"count\\\", \\\"type\\\": \\\"quantitative\\\"}\\n            }\\n        }\\n    },\\n    {\\n        \\\"natural_language_query\\\": \\\"Of these, which is Jetblue Airways?\\\",\\n        \\\"visualization_code\\\": {\\n            \\\"$schema\\\": \\\"https://vega.github.io/schema/vega-lite/v5.json\\\",\\n            \\\"data\\\": {\\n                \\\"values\\\": [\\n                    {\\\"Airline\\\": \\\"JetBlue Airways\\\", \\\"Country\\\": \\\"United States\\\"}\\n                ]\\n            },\\n            \\\"mark\\\": \\\"bar\\\",\\n            \\\"encoding\\\": {\\n                \\\"x\\\": {\\\"field\\\": \\\"Airline\\\", \\\"type\\\": \\\"nominal\\\"},\\n                \\\"y\\\": {\\\"aggregate\\\": \\\"count\\\", \\\"type\\\": \\\"quantitative\\\"}\\n            },\\n            \\\"transform\\\": [\\n                {\\\"filter\\\": {\\\"field\\\": \\\"Airline\\\", \\\"equal\\\": \\\"JetBlue Airways\\\"}}\\n            ]\\n        }\\n    },\\n    {\\n        \\\"natural_language_query\\\": \\\"What is the country corresponding it?\\\",\\n        \\\"visualization_code\\\": {\\n            \\\"$schema\\\": \\\"https://vega.github.io/schema/vega-lite/v5.json\\\",\\n            \\\"data\\\": {\\n                \\\"values\\\": [\\n                    {\\\"Country\\\": \\\"United States\\\"}\\n                ]\\n            },\\n            \\\"mark\\\": \\\"bar\\\",\\n            \\\"encoding\\\": {\\n                \\\"x\\\": {\\\"field\\\": \\\"Country\\\", \\\"type\\\": \\\"nominal\\\"},\\n                \\\"y\\\": {\\\"aggregate\\\": \\\"count\\\", \\\"type\\\": \\\"quantitative\\\"}\\n            },\\n            \\\"transform\\\": [\\n                {\\\"filter\\\": {\\\"field\\\": \\\"Airline\\\", \\\"equal\\\": \\\"JetBlue Airways\\\"}},\\n                {\\\"select\\\": [\\\"Country\\\"]}\\n            ]\\n        }\\n    }\\n]\"\n",
        "# instruction=\"You are a data synthesizer to generate data from natural langugae to visualization code based on the following iteractions and database schema. Please follow these steps: 1. First create some data for the given database schema, the queries should be taken into account to make related data for visualization. 2. Use the data to create corresponding vega-lite visualizations based on the given interactions of natural language questions and SQL queries. 3. Ouput the data in a json format as a list of dictionaries which have two features: natural language query and the visualization code. The visualization code should include actual data. Please give ONLY the json format without any text explanations.\"\n",
        "# prompt1=\"Please first create some data for the given database schema, the queries should be taken into account to make related data for visualization.\"\n",
        "# prompt2=\"Please use the data to create corresponding vega-lite visualizations based on the given interactions of natural language questions and SQL queries.\"\n",
        "# prompt3=\"Ouput the data in a json format as a list of dictionaries which have two features: natural language description and the visualization code. The visualization code should include actual data. Please give ONLY the json format without any text explanations.\"\n",
        "results,predictions = [],[]\n",
        "pattern = r\"```(.*?)```\"\n",
        "\n",
        "for interaction in interactions[0:100]: \n",
        "    messages = [{\"role\": \"system\", \"content\": instruction},{\"role\": \"system\", \"content\": example},{\"role\": \"user\", \"content\": interaction}] \n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        temperature=0,\n",
        "        max_tokens=2000,\n",
        "        timeout=20,\n",
        "        \n",
        "    )\n",
        "    chat_response= completion.choices[0].message.content\n",
        "    print(chat_response)\n",
        "    json_matches = re.findall(pattern, chat_response, re.DOTALL)\n",
        "    if not json_matches:\n",
        "        lst=eval(chat_response)\n",
        "    else:\n",
        "        lst=eval(json_matches[0].strip().replace(\"json\", \"\"))\n",
        "    results.append({\"input\": interaction, \"output\": lst})\n",
        "    predictions.append(lst)\n",
        "print(len(predictions))\n",
        "with open(\"result_vis_interactions.json\", \"w\") as file:\n",
        "    json.dump(results, file, indent=4)\n",
        "with open(\"predictions_vis_interactions.json\", \"w\") as file:\n",
        "    json.dump(predictions, file, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(len(predictions))\n",
        "with open(\"predictions_vis_interactions.json\", \"w\") as file:\n",
        "    json.dump(predictions, file, indent=4)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### part 2: schema-only + example+ loop+loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"conversation\": [\n",
            "        {\n",
            "            \"query\": \"Which airlines operate flights from a particular airport?\",\n",
            "            \"vis_code\": {\n",
            "                \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",\n",
            "                \"data\": {\n",
            "                    \"url\": \"https://flight_2.com/flights\"\n",
            "                },\n",
            "                \"transform\": [\n",
            "                    {\"lookup\": \"sourceairport\", \"from\": {\"data\": {\"url\": \"https://flight_2.com/airports\"}}, \"as\": \"sourceairport_info\"},\n",
            "                    {\"lookup\": \"destairport\", \"from\": {\"data\": {\"url\": \"https://flight_2.com/airports\"}}, \"as\": \"destairport_info\"},\n",
            "                    {\"filter\": {\"field\": \"sourceairport_info.airportcode\", \"equal\": \"LAX\"}},\n",
            "                    {\"join\": {\"with\": \"airlines\", \"type\": \"inner\", \"key\": \"uid\", \"as\": \"airlines\"}},\n",
            "                    {\"aggregate\": [{\"op\": \"count\", \"as\": \"Number of Airlines\"}], \"groupby\": [\"sourceairport_info.city\"]}                        \n",
            "                ],\n",
            "                \"mark\": \"bar\",\n",
            "                \"encoding\": {\n",
            "                    \"y\": {\"field\": \"Number of Airlines\", \"type\": \"quantitative\"},\n",
            "                    \"x\": {\"field\": \"sourceairport_info.city\", \"type\": \"nominal\"},\n",
            "                    \"color\": {\"field\": \"sourceairport_info.countryabbrev\", \"type\": \"nominal\"}                  \n",
            "                }\n",
            "            }\n",
            "        },\n",
            "        {\n",
            "            \"query\": \"What is the total number of flights operated by each airline?\",\n",
            "            \"vis_code\": {\n",
            "                \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",\n",
            "                \"data\": {\n",
            "                    \"url\": \"https://flight_2.com/flights\"\n",
            "                },\n",
            "                \"transform\": [\n",
            "                    {\"join\": {\"with\": \"airlines\", \"type\": \"inner\", \"key\": \"uid\"}},\n",
            "                    {\"aggregate\": [{\"op\": \"count\", \"as\": \"Total Number of Flights\"}], \"groupby\": [\"airlines.airline\"]}                        \n",
            "                ],\n",
            "                \"mark\": \"bar\",\n",
            "                \"encoding\": {\n",
            "                    \"y\": {\"field\": \"Total Number of Flights\", \"type\": \"quantitative\"},\n",
            "                    \"x\": {\"field\": \"airlines.airline\", \"type\": \"nominal\"},\n",
            "                    \"color\": {\"field\": \"airlines.country\", \"type\": \"nominal\"}                  \n",
            "                }\n",
            "            }\n",
            "        },\n",
            "        {\n",
            "            \"query\": \"How many flights are there from city A to city B?\",\n",
            "            \"vis_code\": {\n",
            "                \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",\n",
            "                \"data\": {\n",
            "                    \"url\": \"https://flight_2.com/flights\"\n",
            "                },\n",
            "                \"transform\": [\n",
            "                    {\"lookup\": \"sourceairport\", \"from\": {\"data\": {\"url\": \"https://flight_2.com/airports\"}}, \"as\": \"sourceairport_info\"},\n",
            "                    {\"lookup\": \"destairport\", \"from\": {\"data\": {\"url\": \"https://flight_2.com/airports\"}}, \"as\": \"destairport_info\"},\n",
            "                    {\"filter\": {\"and\": [{\"field\": \"sourceairport_info.city\", \"equal\": \"City A\"}, {\"field\": \"destairport_info.city\", \"equal\": \"City B\"}]}},\n",
            "                    {\"join\": {\"with\": \"airlines\", \"type\": \"inner\", \"key\": \"uid\"}}                        \n",
            "                ],\n",
            "                \"mark\": \"bar\",\n",
            "                \"encoding\": {\n",
            "                    \"y\": {\"field\": \"*\", \"type\": \"quantitative\", \"aggregate\": \"sum\"},\n",
            "                    \"x\": {\"field\": \"airlines.abbreviation\", \"type\": \"nominal\"},\n",
            "                    \"color\": {\"field\": \"airlines.country\", \"type\": \"nominal\"}                  \n",
            "                }\n",
            "            }\n",
            "        }\n",
            "    ]\n",
            "}\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "### nl2vis visualization generation from schema\n",
        "\n",
        "import openai\n",
        "import json\n",
        "openai.api_key = \"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\"\n",
        "with open('schema_str.json', 'r') as file:\n",
        "    schemas = json.load(file)\n",
        "\n",
        "instruction=\"You are a data synthesizer to generate data from natural langugae to visualization code based on the following database schema. Please follow these steps: 1. Create a conversation between user and bot of natural language questions and corresponding vega-lite visualizations. Make 2-4 rounds of interactions and try to form a logically coherent conversation. 2. Ouput the conversation in a json format as a list of dictionaries which have two features: natural language query and the visualization code. The visualization code should include some actual data. Try to make diverse visualization types such as bar charts, line charts, scatter charts and so on. Please give ONLY the json format without any text explanations.\"\n",
        "# instruction=\"You are a data synthesizer to generate data from natural langugae to visualization code based on the following database schema. Please follow these steps: 1. First create some data for the given database schema to be used to make visualizations later.2. Use the data to create some natural language questions and corresponding vega-lite visualizations. Try to follow a conversational manner and form a logically coherent interaction. Make 3-5 rounds of interactions in the conversation.(don't output) 3. Ouput the data in a json format as a list of dictionaries which have two features: natural language description and the visualization code. The visualization code should include actual data. Please give ONLY the json format without any text explanations.\"\n",
        "# prompt1=\"Please first create some data for the given database schema to be used to make visualizations later.\"\n",
        "# prompt2=\"Please use the data to create some natural language questions and corresponding vega-lite visualizations. Try to follow a conversational manner and form a logically coherent interaction. Make 3-5 rounds of interactions in the conversation.\"\n",
        "# prompt3=\"Ouput the data in a json format as a list of dictionaries which have two features: natural language description and the visualization code. The visualization code should include actual data. Please give ONLY the json format without any text explanations\"\n",
        "\n",
        "predictions =[]\n",
        "\n",
        "for schema in schemas[0:1]: \n",
        "    messages = [{\"role\": \"system\", \"content\": instruction},{\"role\": \"user\", \"content\": schema}] \n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        temperature=1,\n",
        "        max_tokens=2000,\n",
        "        timeout=20,\n",
        "        \n",
        "    )\n",
        "    chat_response= completion.choices[0].message.content\n",
        "    print(chat_response)\n",
        "    lst=eval(chat_response)\n",
        "    results.append({\"input\": interaction, \"output\": lst})\n",
        "    predictions.append(lst)\n",
        "print(len(predictions))\n",
        "with open(\"predictions_vis_schema.json\", \"w\") as file:\n",
        "    json.dump(predictions, file, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here is some sample data for the given database schema:\n",
            "\n",
            "```\n",
            "airlines:\n",
            "[\n",
            "  {\"uid\": 1, \"airline\": \"Delta Airlines\", \"abbreviation\": \"DL\", \"country\": \"USA\"},\n",
            "  {\"uid\": 2, \"airline\": \"United Airlines\", \"abbreviation\": \"UA\", \"country\": \"USA\"},\n",
            "  {\"uid\": 3, \"airline\": \"British Airways\", \"abbreviation\": \"BA\", \"country\": \"UK\"},\n",
            "  {\"uid\": 4, \"airline\": \"Air France\", \"abbreviation\": \"AF\", \"country\": \"France\"},\n",
            "  {\"uid\": 5, \"airline\": \"Lufthansa\", \"abbreviation\": \"LH\", \"country\": \"Germany\"}\n",
            "]\n",
            "\n",
            "airports:\n",
            "[\n",
            "  {\"city\": \"New York\", \"airportcode\": \"JFK\", \"airportname\": \"John F. Kennedy International Airport\", \"country\": \"USA\", \"countryabbrev\": \"US\"},\n",
            "  {\"city\": \"Los Angeles\", \"airportcode\": \"LAX\", \"airportname\": \"Los Angeles International Airport\", \"country\": \"USA\", \"countryabbrev\": \"US\"},\n",
            "  {\"city\": \"London\", \"airportcode\": \"LHR\", \"airportname\": \"Heathrow Airport\", \"country\": \"UK\", \"countryabbrev\": \"UK\"},\n",
            "  {\"city\": \"Paris\", \"airportcode\": \"CDG\", \"airportname\": \"Charles de Gaulle Airport\", \"country\": \"France\", \"countryabbrev\": \"FR\"},\n",
            "  {\"city\": \"Frankfurt\", \"airportcode\": \"FRA\", \"airportname\": \"Frankfurt Airport\", \"country\": \"Germany\", \"countryabbrev\": \"DE\"}\n",
            "]\n",
            "\n",
            "flights:\n",
            "[\n",
            "  {\"airline\": 1, \"flightno\": \"DL101\", \"sourceairport\": \"JFK\", \"destairport\": \"LAX\"},\n",
            "  {\"airline\": 2, \"flightno\": \"UA202\", \"sourceairport\": \"LAX\", \"destairport\": \"JFK\"},\n",
            "  {\"airline\": 3, \"flightno\": \"BA303\", \"sourceairport\": \"LHR\", \"destairport\": \"CDG\"},\n",
            "  {\"airline\": 4, \"flightno\": \"AF404\", \"sourceairport\": \"CDG\", \"destairport\": \"FRA\"},\n",
            "  {\"airline\": 5, \"flightno\": \"LH505\", \"sourceairport\": \"FRA\", \"destairport\": \"LHR\"}\n",
            "]\n",
            "```\n",
            "\n",
            "Here's an example conversation with corresponding vega-lite visualizations:\n",
            "\n",
            "User: Can you show me a bar chart of the number of flights per airline?\n",
            "\n",
            "Bot: Sure, here you go.\n",
            "\n",
            "```\n",
            "{\n",
            "  \"description\": \"A bar chart showing the number of flights per airline\",\n",
            "  \"mark\": \"bar\",\n",
            "  \"encoding\": {\n",
            "    \"x\": {\"field\": \"airline\", \"type\": \"nominal\"},\n",
            "    \"y\": {\"aggregate\": \"count\", \"type\": \"quantitative\"}\n",
            "  },\n",
            "  \"data\": {\n",
            "    \"values\": [\n",
            "      {\"airline\": \"Delta Airlines\"},\n",
            "      {\"airline\": \"United Airlines\"},\n",
            "      {\"airline\": \"British Airways\"},\n",
            "      {\"airline\": \"Air France\"},\n",
            "      {\"airline\": \"Lufthansa\"}\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "```\n",
            "\n",
            "User: Can you show me a map of the airports with the number of flights originating from each airport?\n",
            "\n",
            "Bot: Sure, here you go.\n",
            "\n",
            "```\n",
            "{\n",
            "  \"description\": \"A map showing the number of flights originating from each airport\",\n",
            "  \"layer\": [\n",
            "    {\n",
            "      \"mark\": {\"type\": \"circle\", \"opacity\": 0.8, \"stroke\": \"black\", \"strokeWidth\": 1},\n",
            "      \"encoding\": {\n",
            "        \"longitude\": {\"field\": \"longitude\", \"type\": \"quantitative\"},\n",
            "        \"latitude\": {\"field\": \"latitude\", \"type\": \"quantitative\"},\n",
            "        \"size\": {\"aggregate\": \"count\", \"type\": \"quantitative\"},\n",
            "        \"color\": {\"aggregate\": \"count\", \"type\": \"quantitative\"}\n",
            "      },\n",
            "      \"data\": {\n",
            "        \"url\": \"https://vega.github.io/vega-datasets/data/world-110m.json\",\n",
            "        \"format\": {\"type\": \"topojson\", \"feature\": \"countries\"}\n",
            "      }\n",
            "    },\n",
            "    {\n",
            "      \"mark\": {\"type\": \"text\", \"align\": \"center\", \"baseline\": \"middle\", \"dx\": 0, \"dy\": 0},\n",
            "      \"encoding\": {\n",
            "        \"longitude\": {\"field\": \"longitude\", \"type\": \"quantitative\"},\n",
            "        \"latitude\": {\"field\": \"latitude\", \"type\": \"quantitative\"},\n",
            "        \"text\": {\"field\": \"airportname\", \"type\": \"nominal\"},\n",
            "        \"size\": {\"value\": 10},\n",
            "        \"color\": {\"value\": \"black\"}\n",
            "      },\n",
            "      \"data\": {\n",
            "        \"values\": [\n",
            "          {\"airportcode\": \"JFK\", \"airportname\": \"John F. Kennedy International Airport\", \"latitude\": 40.6397, \"longitude\": -73.7789},\n",
            "          {\"airportcode\": \"LAX\", \"airportname\": \"Los Angeles International Airport\", \"latitude\": 33.9425, \"longitude\": -118.4081},\n",
            "          {\"airportcode\": \"LHR\", \"airportname\": \"Heathrow Airport\", \"latitude\": 51.4700, \"longitude\": -0.4543},\n",
            "          {\"airportcode\": \"CDG\", \"airportname\": \"Charles de Gaulle Airport\", \"latitude\": 49.0097, \"longitude\": 2.5479},\n",
            "          {\"airportcode\": \"FRA\", \"airportname\": \"Frankfurt Airport\", \"latitude\": 50.0379, \"longitude\": 8.5622}\n",
            "        ]\n",
            "      }\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "```\n",
            "\n",
            "User: Can you show me a scatter plot of the flight distances between each airport?\n",
            "\n",
            "Bot: Sure, here you go.\n",
            "\n",
            "```\n",
            "{\n",
            "  \"description\": \"A scatter plot showing the flight distances between each airport\",\n",
            "  \"mark\": \"point\",\n",
            "  \"encoding\": {\n",
            "    \"x\": {\"field\": \"sourceairport\", \"type\": \"nominal\"},\n",
            "    \"y\": {\"field\": \"destairport\", \"type\": \"nominal\"},\n",
            "    \"size\": {\"field\": \"distance\", \"type\": \"quantitative\"}\n",
            "  },\n",
            "  \"data\": {\n",
            "    \"values\": [\n",
            "      {\"sourceairport\": \"JFK\", \"destairport\": \"LAX\", \"distance\": 2475},\n",
            "      {\"sourceairport\": \"LAX\", \"destairport\": \"JFK\", \"distance\": 2475},\n",
            "      {\"sourceairport\": \"LHR\", \"destairport\": \"CDG\", \"distance\": 214},\n",
            "      {\"sourceairport\": \"CDG\", \"destairport\": \"FRA\", \"distance\": 393},\n",
            "      {\"sourceairport\": \"FRA\", \"destairport\": \"LHR\", \"distance\": 396}\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "``` \n",
            "\n",
            "User: Can you show me a pie chart of the percentage of flights per country?\n",
            "\n",
            "Bot: Sure, here you go.\n",
            "\n",
            "```\n",
            "{\n",
            "  \"description\": \"A pie chart showing the percentage of flights per country\",\n",
            "  \"mark\": \"arc\",\n",
            "  \"encoding\": {\n",
            "    \"theta\": {\"aggregate\": \"sum\", \"field\": \"count\", \"type\": \"quantitative\"},\n",
            "    \"color\": {\"field\": \"country\", \"type\": \"nominal\"}\n",
            "  },\n",
            "  \"data\": {\n",
            "    \"values\": [\n",
            "      {\"airline\": \"Delta Airlines\", \"count\": 1, \"country\": \"USA\"},\n",
            "      {\"airline\": \"United Airlines\", \"count\": 1, \"country\": \"USA\"},\n",
            "      {\"airline\": \"British Airways\", \"count\": 1, \"country\": \"UK\"},\n",
            "      {\"airline\": \"Air France\", \"count\": 1, \"country\": \"France\"},\n",
            "      {\"airline\": \"Lufthansa\", \"count\": 1, \"country\": \"Germany\"}\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "``` \n",
            "\n",
            "User: Can you show me a stacked bar chart of the number of flights per airline per country?\n",
            "\n",
            "Bot: Sure, here you go.\n",
            "\n",
            "```\n",
            "{\n",
            "  \"description\": \"A stacked bar chart showing the number of flights per airline per country\",\n",
            "  \"mark\": \"bar\",\n",
            "  \"encoding\": {\n",
            "    \"x\": {\"field\": \"airline\", \"type\": \"nominal\"},\n",
            "    \"y\": {\"aggregate\": \"count\", \"type\": \"quantitative\"},\n",
            "    \"color\": {\"field\": \"country\", \"type\": \"nominal\"}\n",
            "  },\n",
            "  \"data\": {\n",
            "    \"values\": [\n",
            "      {\"airline\": \"Delta Airlines\", \"country\": \"USA\"},\n",
            "      {\"airline\": \"United Airlines\", \"country\": \"USA\"},\n",
            "      {\"airline\": \"British Airways\", \"country\": \"UK\"},\n",
            "      {\"airline\": \"Air France\", \"country\": \"France\"},\n",
            "      {\"airline\": \"Lufthansa\", \"country\": \"Germany\"}\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "``` \n",
            "\n",
            "Here is the data in json format:\n",
            "\n",
            "```\n",
            "[\n",
            "  {\n",
            "    \"description\": \"A bar chart showing the number of flights per airline\",\n",
            "    \"mark\": \"bar\",\n",
            "    \"encoding\": {\n",
            "      \"x\": {\"field\": \"airline\", \"type\": \"nominal\"},\n",
            "      \"y\": {\"aggregate\": \"count\", \"type\": \"quantitative\"}\n",
            "    },\n",
            "    \"data\": {\n",
            "      \"values\": [\n",
            "        {\"airline\": \"\n"
          ]
        }
      ],
      "source": [
        "print(chat_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(len(predictions))\n",
        "with open(\"predictions_vis_interactions.json\", \"w\") as file:\n",
        "    json.dump(predictions, file, indent=4)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OrnkXVb1BBq4"
      },
      "source": [
        "# NL2SQL"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kpd4bPDKeqhf"
      },
      "source": [
        "## Spider experiment"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ncCVu1syArrR"
      },
      "source": [
        "### preparations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5Cado75GzNz",
        "outputId": "a3017b0d-1ba7-4aeb-b4c9-ab3a4c8ad034"
      },
      "outputs": [],
      "source": [
        "### Spider experiment\n",
        "\n",
        "instruction=\"You are a semantic parser to translate natural language question to SQL query. In each question, the database schema will be provided. In the SQL you don't need to indicate the name of the database, just write SQL within the database\"\n",
        "messages = [\n",
        " {\"role\": \"system\", \"content\" : instruction}\n",
        "]\n",
        "content = \"question: List the creation year, name and budget of each department. serialized_schema: | department_management | department : department_id , name , creation , ranking , budget_in_billions , num_employees | head : head_id , name , born_state , age | management : department_id , head_id , temporary_acting\"\n",
        "messages.append({\"role\": \"user\", \"content\": content})\n",
        "\n",
        "completion = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=messages\n",
        ")\n",
        "\n",
        "chat_response = completion.choices[0].message.content\n",
        "print(f'ChatGPT: {chat_response}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKIvlkI-fWyL",
        "outputId": "4319c37a-1e61-42b4-bf43-3e88895fd719"
      },
      "outputs": [],
      "source": [
        "### load the dataset file\n",
        "import json\n",
        "# Open the JSON file in read mode\n",
        "with open('serialized_data_str.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "    print(len(data))\n",
        "    print(type(data))\n",
        "    print(type(data[0]))\n",
        "    # for item in data:\n",
        "    #     print(item)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFbKU-9AgF0P",
        "outputId": "7911d933-0746-4fee-82b5-3d32ba92d706"
      },
      "outputs": [],
      "source": [
        "### process one item of dataset\n",
        "\n",
        "instruction=\"You are a semantic parser to translate natural language question to SQL query. In each question, the database schema will be provided. In the SQL you don't need to indicate the name of the database, just write SQL within the database\"\n",
        "messages = [\n",
        " {\"role\": \"system\", \"content\" : instruction}\n",
        "]\n",
        "content = data[0]\n",
        "messages.append({\"role\": \"user\", \"content\": content})\n",
        "\n",
        "completion = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=messages,\n",
        "  max_tokens=100,\n",
        "  temperature=0\n",
        ")\n",
        "\n",
        "chat_response = completion.choices[0].message.content\n",
        "print(chat_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fkr8lXkDABcd"
      },
      "outputs": [],
      "source": [
        "### process the whole dataset (questions and schemas separated)\n",
        "\n",
        "import openai\n",
        "import json\n",
        "\n",
        "# Define the natural questions as a list\n",
        "questions = [\"What are the names of all the employees in the HR department?\",\n",
        "             \"What is the total revenue for each product category?\",\n",
        "             \"Which customers have made more than 5 purchases in the past month?\"]\n",
        "\n",
        "# Define the database schema for each question\n",
        "schemas = [\"CREATE TABLE Employees (id INT, name TEXT, department TEXT)\",\n",
        "           \"CREATE TABLE Sales (product TEXT, revenue INT)\",\n",
        "           \"CREATE TABLE Customers (id INT, name TEXT, email TEXT, purchases INT)\"]\n",
        "\n",
        "# Define the instruction for the semantic parser\n",
        "instruction = \"You are a semantic parser to translate natural language question to SQL query. In each question, the database schema will be provided. In the SQL you don't need to indicate the name of the database, just write SQL within the database\"\n",
        "\n",
        "# Initialize the messages list with the instruction\n",
        "messages = [{\"role\": \"system\", \"content\" : instruction}]\n",
        "\n",
        "# Loop through each question and database schema\n",
        "for i in range(len(questions)):\n",
        "    # Add the question and schema to the messages list\n",
        "    messages.append({\"role\": \"user\", \"content\": questions[i]})\n",
        "    messages.append({\"role\": \"system\", \"content\": schemas[i]})\n",
        "    \n",
        "    # Generate the SQL query using the OpenAI API\n",
        "    completion = openai.Completion.create(\n",
        "        engine=\"text-davinci-002\",\n",
        "        prompt=\"\\n\".join([m[\"content\"] for m in messages]),\n",
        "        max_tokens=1024,\n",
        "        temperature=0.5,\n",
        "        n = 1,\n",
        "        stop=None,\n",
        "        timeout=15,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "    \n",
        "    # Get the SQL query from the API response\n",
        "    chat_response = completion.choices[0].text.strip()\n",
        "    \n",
        "    # Add the SQL query to the messages list\n",
        "    messages.append({\"role\": \"system\", \"content\": chat_response})\n",
        "    \n",
        "# Create a list of dictionaries to hold the results\n",
        "results = []\n",
        "\n",
        "# Loop through each question and SQL query\n",
        "for i in range(0, len(messages), 3):\n",
        "    # Extract the question and SQL query from the messages list\n",
        "    question = messages[i+1][\"content\"]\n",
        "    sql_query = messages[i+2][\"content\"]\n",
        "    \n",
        "    # Add the question and SQL query to the results list as a dictionary\n",
        "    results.append({\"question\": question, \"sql_query\": sql_query})\n",
        "\n",
        "# Save the results to a JSON file\n",
        "with open(\"results.json\", \"w\") as file:\n",
        "    json.dump(results, file, indent=4)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "juw50ca5AxEj"
      },
      "source": [
        "### Spider"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLnNO5AVYH0t"
      },
      "outputs": [],
      "source": [
        "### Spider, text-davinci-002\n",
        "\n",
        "import openai\n",
        "import json\n",
        "import files\n",
        "# question_schemas = [\"What are the names of all the employees in the HR department?|CREATE TABLE Employees (id INT, name TEXT, department TEXT)\"]\n",
        "with open('serialized_data_str.json', 'r') as file:\n",
        "    question_schemas = json.load(file)\n",
        "\n",
        "instruction = \"You are a semantic parser to translate natural language question to SQL query. In each question, the database schema will be provided. In the SQL you don't need to indicate the name of the database, just write SQL within the database. Please give only the SQL queries without any text explanations\"\n",
        "example = \"Here is an example including input and output. Input: question: How many heads of the departments are older than 56 ? serialized_schema:  | department_management | department : department_id , name , creation , ranking , budget_in_billions , num_employees | head : head_id , name , born_state , age | management : department_id , head_id , temporary_acting. Output: select count(*) from head where age > 56\"\n",
        "#messages = [{\"role\": \"system\", \"content\": instruction}]\n",
        "results,predictions = [],[]\n",
        "\n",
        "for question_schema in question_schemas: \n",
        "    messages = [{\"role\": \"system\", \"content\": instruction}]\n",
        "    #messages = [{\"role\": \"system\", \"content\": instruction},{\"role\": \"system\", \"content\": example}]   \n",
        "    messages.append({\"role\": \"user\", \"content\": question_schema})\n",
        "    completion = openai.Completion.create(\n",
        "        engine=\"text-davinci-002\",\n",
        "        prompt=\"\".join([m[\"content\"] for m in messages]),\n",
        "        max_tokens=200,\n",
        "        temperature=0,\n",
        "        n = 1,\n",
        "        stop=None,\n",
        "        timeout=15,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "    chat_response = completion.choices[0].text.strip()\n",
        "    messages.append({\"role\": \"assistant\", \"content\": chat_response}) \n",
        "    sql_query = chat_response.replace(\"\\n\", \" \")\n",
        "    print(sql_query)\n",
        "\n",
        "    results.append({\"input\": question_schema, \"sql_query\": sql_query})\n",
        "    predictions.append(sql_query)\n",
        "\n",
        "# for i in range(1, len(messages)-1, 2):\n",
        "#     question = messages[i][\"content\"]\n",
        "#     sql_query = messages[i+1][\"content\"].replace(\"\\n\", \"\")\n",
        "#     results.append({\"input\": question, \"sql_query\": sql_query})\n",
        "\n",
        "# with open(\"results_text_davinci_003.json\", \"w\") as file:\n",
        "#     json.dump(results, file, indent=4)\n",
        "with open(\"predictions_text_davinci_002.json\", \"w\") as file:\n",
        "    json.dump(predictions, file, indent=4)\n",
        "\n",
        "files.download(\"predictions_text_davinci_002.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoDHgIXTAi2U"
      },
      "outputs": [],
      "source": [
        "### Spider, text-davinci-003\n",
        "\n",
        "import openai\n",
        "import json\n",
        "from google.colab import files\n",
        "# question_schemas = [\"What are the names of all the employees in the HR department?|CREATE TABLE Employees (id INT, name TEXT, department TEXT)\"]\n",
        "with open('serialized_data_str.json', 'r') as file:\n",
        "    question_schemas = json.load(file)\n",
        "\n",
        "instruction = \"You are a semantic parser to translate natural language question to SQL query. In each question, the database schema will be provided. In the SQL you don't need to indicate the name of the database, just write SQL within the database. Please give only the SQL queries without any text explanations\"\n",
        "example = \"Here is an example including input and output. Input: question: How many heads of the departments are older than 56 ? serialized_schema:  | department_management | department : department_id , name , creation , ranking , budget_in_billions , num_employees | head : head_id , name , born_state , age | management : department_id , head_id , temporary_acting. Output: select count(*) from head where age > 56\"\n",
        "#messages = [{\"role\": \"system\", \"content\": instruction}]\n",
        "results,predictions = [],[]\n",
        "\n",
        "for question_schema in question_schemas: \n",
        "    messages = [{\"role\": \"system\", \"content\": instruction}]\n",
        "    #messages = [{\"role\": \"system\", \"content\": instruction},{\"role\": \"system\", \"content\": example}]   \n",
        "    messages.append({\"role\": \"user\", \"content\": question_schema})\n",
        "    completion = openai.Completion.create(\n",
        "        engine=\"text-davinci-003\",\n",
        "        prompt=\"\".join([m[\"content\"] for m in messages]),\n",
        "        max_tokens=200,\n",
        "        temperature=0,\n",
        "        n = 1,\n",
        "        stop=None,\n",
        "        timeout=15,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "    chat_response = completion.choices[0].text.strip()\n",
        "    messages.append({\"role\": \"assistant\", \"content\": chat_response}) \n",
        "    sql_query = chat_response.replace(\"\\n\", \" \")\n",
        "    print(sql_query)\n",
        "\n",
        "    results.append({\"input\": question_schema, \"sql_query\": sql_query})\n",
        "    predictions.append(sql_query)\n",
        "\n",
        "# for i in range(1, len(messages)-1, 2):\n",
        "#     question = messages[i][\"content\"]\n",
        "#     sql_query = messages[i+1][\"content\"].replace(\"\\n\", \"\")\n",
        "#     results.append({\"input\": question, \"sql_query\": sql_query})\n",
        "\n",
        "# with open(\"results_text_davinci_003.json\", \"w\") as file:\n",
        "#     json.dump(results, file, indent=4)\n",
        "\n",
        "with open(\"predictions_text_davinci_003.json\", \"w\") as file:\n",
        "    json.dump(predictions, file, indent=4)\n",
        "\n",
        "files.download(\"predictions_text_davinci_003.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WTdwXyNRHlY"
      },
      "outputs": [],
      "source": [
        "### Spider, gpt-3.5-turbo\n",
        "\n",
        "import openai\n",
        "import json\n",
        "from google.colab import files\n",
        "# question_schemas = [\"What are the names of all the employees in the HR department?|CREATE TABLE Employees (id INT, name TEXT, department TEXT)\"]\n",
        "with open('serialized_data_str.json', 'r') as file:\n",
        "    question_schemas = json.load(file)\n",
        "\n",
        "instruction = \"You are a semantic parser to translate natural language question to SQL query. In each question, the database schema will be provided. In the SQL you don't need to indicate the name of the database, just write SQL within the database. Please give only the SQL queries without any text explanations\"\n",
        "example = \"Here is an example including input and output. Input: question: How many heads of the departments are older than 56 ? serialized_schema:  | department_management | department : department_id , name , creation , ranking , budget_in_billions , num_employees | head : head_id , name , born_state , age | management : department_id , head_id , temporary_acting. Output: select count(*) from head where age > 56\"\n",
        "example_2 = \"Here is another example including input and output. Input: question: List the name, born state and age of the heads of departments ordered by age. serialized_schema:  | department_management | department : department_id , name , creation , ranking , budget_in_billions , num_employees | head : head_id , name , born_state , age | management : department_id , head_id , temporary_acting. Output: select name, born_state, age from head order by age\"\n",
        "\n",
        "#messages = [{\"role\": \"system\", \"content\": instruction}]\n",
        "\n",
        "results,predictions = [],[]\n",
        "\n",
        "for question_schema in question_schemas: \n",
        "    #messages = [{\"role\": \"system\", \"content\": instruction}]\n",
        "    messages = [{\"role\": \"system\", \"content\": instruction},{\"role\": \"system\", \"content\": example}] \n",
        "    #messages = [{\"role\": \"system\", \"content\": instruction},{\"role\": \"system\", \"content\": example},{\"role\": \"system\", \"content\": example_2}] \n",
        "    messages.append({\"role\": \"user\", \"content\": question_schema})\n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        max_tokens=200,\n",
        "        temperature=0,\n",
        "        n = 1,\n",
        "        stop=None,\n",
        "        timeout=15,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "    chat_response= completion.choices[0].message.content\n",
        "    sql_query=chat_response.replace(\"\\n\", \" \")\n",
        "\n",
        "    print(sql_query)\n",
        "    messages.append({\"role\": \"assistant\", \"content\": chat_response}) \n",
        "    results.append({\"input\": question_schema, \"sql_query\": sql_query})\n",
        "    predictions.append(sql_query)\n",
        "\n",
        "# with open(\"results_gpt_3.5_turbo.json\", \"w\") as file:\n",
        "#     json.dump(results, file, indent=4)\n",
        "with open(\"predictions_gpt_3.5_turbo_ex_2.json\", \"w\") as file:\n",
        "    json.dump(predictions, file, indent=4)\n",
        "files.download(\"predictions_gpt_3.5_turbo_ex_2.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TbjOUb3NU66",
        "outputId": "ec578bcf-c63c-4ba0-fe89-2f6898ea1bea"
      },
      "outputs": [],
      "source": [
        "print(len(predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Cdx-X1XKeFDk",
        "outputId": "6a9f70dc-aee2-489e-89bc-84c2fa5b2c64"
      },
      "outputs": [],
      "source": [
        "with open(\"predictions_gpt_3.5_turbo_ex_2.json\", \"w\") as file:\n",
        "    json.dump(predictions, file, indent=4)\n",
        "files.download(\"predictions_gpt_3.5_turbo_ex_2.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYfpvrItzSN4",
        "outputId": "28cd3a38-d036-4d15-d348-eaa79d8c4652"
      },
      "outputs": [],
      "source": [
        "with open('predictions_gpt_3.5_turbo_ex.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "    print(len(data))\n",
        "    print(type(data))\n",
        "    print(type(data[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fh2NVW5Zm72_"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2NQ2Mp7Pebpf"
      },
      "source": [
        "### Spider-syn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzFMhfZTefh3"
      },
      "outputs": [],
      "source": [
        "### Spider-syn, gpt-3.5-turbo\n",
        "\n",
        "import openai\n",
        "import json\n",
        "import files\n",
        "# question_schemas = [\"What are the names of all the employees in the HR department?|CREATE TABLE Employees (id INT, name TEXT, department TEXT)\"]\n",
        "with open('serialized_syn_data_str.json', 'r') as file:\n",
        "    question_schemas = json.load(file)\n",
        "\n",
        "instruction = \"You are a semantic parser to translate natural language question to SQL query. In each question, the database schema will be provided. In the SQL you don't need to indicate the name of the database, just write SQL within the database. Please give only the SQL queries without any text explanations\"\n",
        "example = \"Here is an example including input and output. Input: question: How many heads of the departments are older than 56 ? serialized_schema:  | department_management | department : department_id , name , creation , ranking , budget_in_billions , num_employees | head : head_id , name , born_state , age | management : department_id , head_id , temporary_acting. Output: select count(*) from head where age > 56\"\n",
        "example_2 = \"Here is another example including input and output. Input: question: List the name, born state and age of the heads of departments ordered by age. serialized_schema:  | department_management | department : department_id , name , creation , ranking , budget_in_billions , num_employees | head : head_id , name , born_state , age | management : department_id , head_id , temporary_acting. Output: select name, born_state, age from head order by age\"\n",
        "\n",
        "#messages = [{\"role\": \"system\", \"content\": instruction}]\n",
        "\n",
        "results,predictions = [],[]\n",
        "\n",
        "for question_schema in question_schemas: \n",
        "    #messages = [{\"role\": \"system\", \"content\": instruction}]\n",
        "    messages = [{\"role\": \"system\", \"content\": instruction},{\"role\": \"system\", \"content\": example}] \n",
        "    #messages = [{\"role\": \"system\", \"content\": instruction},{\"role\": \"system\", \"content\": example},{\"role\": \"system\", \"content\": example_2}] \n",
        "    messages.append({\"role\": \"user\", \"content\": question_schema})\n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        max_tokens=200,\n",
        "        temperature=0,\n",
        "        n = 1,\n",
        "        stop=None,\n",
        "        timeout=15,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "    chat_response= completion.choices[0].message.content\n",
        "    sql_query=chat_response.replace(\"\\n\", \" \")\n",
        "\n",
        "    print(sql_query)\n",
        "    messages.append({\"role\": \"assistant\", \"content\": chat_response}) \n",
        "    results.append({\"input\": question_schema, \"sql_query\": sql_query})\n",
        "    predictions.append(sql_query)\n",
        "\n",
        "# with open(\"results_gpt_3.5_turbo.json\", \"w\") as file:\n",
        "#     json.dump(results, file, indent=4)\n",
        "with open(\"predictions_gpt_3.5_turbo_syn.json\", \"w\") as file:\n",
        "    json.dump(predictions, file, indent=4)\n",
        "files.download(\"predictions_gpt_3.5_turbo_syn.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AH72k-ZskHTO"
      },
      "outputs": [],
      "source": [
        "### Spider-syn, text-davinci-003\n",
        "\n",
        "import openai\n",
        "import json\n",
        "import files\n",
        "# question_schemas = [\"What are the names of all the employees in the HR department?|CREATE TABLE Employees (id INT, name TEXT, department TEXT)\"]\n",
        "with open('serialized_syn_data_str.json', 'r') as file:\n",
        "    question_schemas = json.load(file)\n",
        "\n",
        "instruction = \"You are a semantic parser to translate natural language question to SQL query. In each question, the database schema will be provided. In the SQL you don't need to indicate the name of the database, just write SQL within the database. Please give only the SQL queries without any text explanations\"\n",
        "example = \"Here is an example including input and output. Input: question: How many heads of the departments are older than 56 ? serialized_schema:  | department_management | department : department_id , name , creation , ranking , budget_in_billions , num_employees | head : head_id , name , born_state , age | management : department_id , head_id , temporary_acting. Output: select count(*) from head where age > 56\"\n",
        "#messages = [{\"role\": \"system\", \"content\": instruction}]\n",
        "results,predictions = [],[]\n",
        "\n",
        "for question_schema in question_schemas: \n",
        "    messages = [{\"role\": \"system\", \"content\": instruction}]\n",
        "    #messages = [{\"role\": \"system\", \"content\": instruction},{\"role\": \"system\", \"content\": example}]   \n",
        "    messages.append({\"role\": \"user\", \"content\": question_schema})\n",
        "    completion = openai.Completion.create(\n",
        "        engine=\"text-davinci-003\",\n",
        "        prompt=\"\".join([m[\"content\"] for m in messages]),\n",
        "        max_tokens=200,\n",
        "        temperature=0,\n",
        "        n = 1,\n",
        "        stop=None,\n",
        "        timeout=15,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "    chat_response = completion.choices[0].text.strip()\n",
        "    messages.append({\"role\": \"assistant\", \"content\": chat_response}) \n",
        "    sql_query = chat_response.replace(\"\\n\", \" \")\n",
        "    print(sql_query)\n",
        "\n",
        "    results.append({\"input\": question_schema, \"sql_query\": sql_query})\n",
        "    predictions.append(sql_query)\n",
        "\n",
        "# for i in range(1, len(messages)-1, 2):\n",
        "#     question = messages[i][\"content\"]\n",
        "#     sql_query = messages[i+1][\"content\"].replace(\"\\n\", \"\")\n",
        "#     results.append({\"input\": question, \"sql_query\": sql_query})\n",
        "\n",
        "# with open(\"results_text_davinci_003.json\", \"w\") as file:\n",
        "#     json.dump(results, file, indent=4)\n",
        "\n",
        "with open(\"predictions_text_davinci_003_syn.json\", \"w\") as file:\n",
        "    json.dump(predictions, file, indent=4)\n",
        "\n",
        "files.download(\"predictions_text_davinci_003_syn.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwEpwlz4ektL",
        "outputId": "521c44ba-8a66-42a9-875b-9b7e37393f4d"
      },
      "outputs": [],
      "source": [
        "print(len(predictions))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PC2_SR3itp9_"
      },
      "source": [
        "### Spider-dk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1fHBRettugN"
      },
      "outputs": [],
      "source": [
        "### Spider-dk, gpt-3.5-turbo\n",
        "\n",
        "import openai\n",
        "import json\n",
        "import files\n",
        "# question_schemas = [\"What are the names of all the employees in the HR department?|CREATE TABLE Employees (id INT, name TEXT, department TEXT)\"]\n",
        "with open('serialized_dk_data_str.json', 'r') as file:\n",
        "    question_schemas = json.load(file)\n",
        "\n",
        "instruction = \"You are a semantic parser to translate natural language question to SQL query. In each question, the database schema will be provided. In the SQL you don't need to indicate the name of the database, just write SQL within the database. Please give only the SQL queries without any text explanations\"\n",
        "example = \"Here is an example including input and output. Input: question: How many heads of the departments are older than 56 ? serialized_schema:  | department_management | department : department_id , name , creation , ranking , budget_in_billions , num_employees | head : head_id , name , born_state , age | management : department_id , head_id , temporary_acting. Output: select count(*) from head where age > 56\"\n",
        "example_2 = \"Here is another example including input and output. Input: question: List the name, born state and age of the heads of departments ordered by age. serialized_schema:  | department_management | department : department_id , name , creation , ranking , budget_in_billions , num_employees | head : head_id , name , born_state , age | management : department_id , head_id , temporary_acting. Output: select name, born_state, age from head order by age\"\n",
        "\n",
        "#messages = [{\"role\": \"system\", \"content\": instruction}]\n",
        "\n",
        "results,predictions = [],[]\n",
        "\n",
        "for question_schema in question_schemas: \n",
        "    #messages = [{\"role\": \"system\", \"content\": instruction}]\n",
        "    messages = [{\"role\": \"system\", \"content\": instruction},{\"role\": \"system\", \"content\": example}] \n",
        "    #messages = [{\"role\": \"system\", \"content\": instruction},{\"role\": \"system\", \"content\": example},{\"role\": \"system\", \"content\": example_2}] \n",
        "    messages.append({\"role\": \"user\", \"content\": question_schema})\n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        max_tokens=200,\n",
        "        temperature=0,\n",
        "        n = 1,\n",
        "        stop=None,\n",
        "        timeout=15,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "    chat_response= completion.choices[0].message.content\n",
        "    sql_query=chat_response.replace(\"\\n\", \" \")\n",
        "\n",
        "    print(sql_query)\n",
        "    messages.append({\"role\": \"assistant\", \"content\": chat_response}) \n",
        "    results.append({\"input\": question_schema, \"sql_query\": sql_query})\n",
        "    predictions.append(sql_query)\n",
        "\n",
        "# with open(\"results_gpt_3.5_turbo.json\", \"w\") as file:\n",
        "#     json.dump(results, file, indent=4)\n",
        "with open(\"predictions_gpt_3.5_turbo_dk.json\", \"w\") as file:\n",
        "    json.dump(predictions, file, indent=4)\n",
        "files.download(\"predictions_gpt_3.5_turbo_dk.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJr48wavzOUl"
      },
      "outputs": [],
      "source": [
        "### Spider-dk, text-davinci-003\n",
        "\n",
        "import openai\n",
        "import json\n",
        "import files\n",
        "# question_schemas = [\"What are the names of all the employees in the HR department?|CREATE TABLE Employees (id INT, name TEXT, department TEXT)\"]\n",
        "with open('serialized_dk_data_str.json', 'r') as file:\n",
        "    question_schemas = json.load(file)\n",
        "\n",
        "instruction = \"You are a semantic parser to translate natural language question to SQL query. In each question, the database schema will be provided. In the SQL you don't need to indicate the name of the database, just write SQL within the database. Please give only the SQL queries without any text explanations\"\n",
        "example = \"Here is an example including input and output. Input: question: How many heads of the departments are older than 56 ? serialized_schema:  | department_management | department : department_id , name , creation , ranking , budget_in_billions , num_employees | head : head_id , name , born_state , age | management : department_id , head_id , temporary_acting. Output: select count(*) from head where age > 56\"\n",
        "#messages = [{\"role\": \"system\", \"content\": instruction}]\n",
        "results,predictions = [],[]\n",
        "\n",
        "for question_schema in question_schemas: \n",
        "    messages = [{\"role\": \"system\", \"content\": instruction}]\n",
        "    #messages = [{\"role\": \"system\", \"content\": instruction},{\"role\": \"system\", \"content\": example}]   \n",
        "    messages.append({\"role\": \"user\", \"content\": question_schema})\n",
        "    completion = openai.Completion.create(\n",
        "        engine=\"text-davinci-003\",\n",
        "        prompt=\"\".join([m[\"content\"] for m in messages]),\n",
        "        max_tokens=200,\n",
        "        temperature=0,\n",
        "        n = 1,\n",
        "        stop=None,\n",
        "        timeout=15,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "    chat_response = completion.choices[0].text.strip()\n",
        "    messages.append({\"role\": \"assistant\", \"content\": chat_response}) \n",
        "    sql_query = chat_response.replace(\"\\n\", \" \")\n",
        "    print(sql_query)\n",
        "\n",
        "    results.append({\"input\": question_schema, \"sql_query\": sql_query})\n",
        "    predictions.append(sql_query)\n",
        "\n",
        "# for i in range(1, len(messages)-1, 2):\n",
        "#     question = messages[i][\"content\"]\n",
        "#     sql_query = messages[i+1][\"content\"].replace(\"\\n\", \"\")\n",
        "#     results.append({\"input\": question, \"sql_query\": sql_query})\n",
        "\n",
        "# with open(\"results_text_davinci_003.json\", \"w\") as file:\n",
        "#     json.dump(results, file, indent=4)\n",
        "\n",
        "with open(\"predictions_text_davinci_003_dk.json\", \"w\") as file:\n",
        "    json.dump(predictions, file, indent=4)\n",
        "\n",
        "files.download(\"predictions_text_davinci_003_dk.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7vCJ7QiuAgU",
        "outputId": "622a3dda-e485-40e5-e1ed-c8ed4f19f9ef"
      },
      "outputs": [],
      "source": [
        "print(len(predictions))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BJM8zWms4gsw"
      },
      "source": [
        "### Spider-realistic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jk-8h8HM4jkA"
      },
      "outputs": [],
      "source": [
        "### Spider-realisic, gpt-3.5-turbo\n",
        "\n",
        "import openai\n",
        "import json\n",
        "import files\n",
        "# question_schemas = [\"What are the names of all the employees in the HR department?|CREATE TABLE Employees (id INT, name TEXT, department TEXT)\"]\n",
        "with open('serialized_real_data_str.json', 'r') as file:\n",
        "    question_schemas = json.load(file)\n",
        "\n",
        "instruction = \"You are a semantic parser to translate natural language question to SQL query. In each question, the database schema will be provided. In the SQL you don't need to indicate the name of the database, just write SQL within the database. Please give only the SQL queries without any text explanations\"\n",
        "example = \"Here is an example including input and output. Input: question: How many heads of the departments are older than 56 ? serialized_schema:  | department_management | department : department_id , name , creation , ranking , budget_in_billions , num_employees | head : head_id , name , born_state , age | management : department_id , head_id , temporary_acting. Output: select count(*) from head where age > 56\"\n",
        "example_2 = \"Here is another example including input and output. Input: question: List the name, born state and age of the heads of departments ordered by age. serialized_schema:  | department_management | department : department_id , name , creation , ranking , budget_in_billions , num_employees | head : head_id , name , born_state , age | management : department_id , head_id , temporary_acting. Output: select name, born_state, age from head order by age\"\n",
        "\n",
        "#messages = [{\"role\": \"system\", \"content\": instruction}]\n",
        "\n",
        "results,predictions = [],[]\n",
        "\n",
        "for question_schema in question_schemas: \n",
        "    #messages = [{\"role\": \"system\", \"content\": instruction}]\n",
        "    messages = [{\"role\": \"system\", \"content\": instruction},{\"role\": \"system\", \"content\": example}] \n",
        "    #messages = [{\"role\": \"system\", \"content\": instruction},{\"role\": \"system\", \"content\": example},{\"role\": \"system\", \"content\": example_2}] \n",
        "    messages.append({\"role\": \"user\", \"content\": question_schema})\n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        max_tokens=200,\n",
        "        temperature=0,\n",
        "        n = 1,\n",
        "        stop=None,\n",
        "        timeout=15,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "    chat_response= completion.choices[0].message.content\n",
        "    sql_query=chat_response.replace(\"\\n\", \" \")\n",
        "\n",
        "    print(sql_query)\n",
        "    messages.append({\"role\": \"assistant\", \"content\": chat_response}) \n",
        "    results.append({\"input\": question_schema, \"sql_query\": sql_query})\n",
        "    predictions.append(sql_query)\n",
        "\n",
        "# with open(\"results_gpt_3.5_turbo.json\", \"w\") as file:\n",
        "#     json.dump(results, file, indent=4)\n",
        "with open(\"predictions_gpt_3.5_turbo_real.json\", \"w\") as file:\n",
        "    json.dump(predictions, file, indent=4)\n",
        "files.download(\"predictions_gpt_3.5_turbo_real.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCKTM98f9k-b"
      },
      "outputs": [],
      "source": [
        "### Spider-realistic, text-davinci-003\n",
        "\n",
        "import openai\n",
        "import json\n",
        "import files\n",
        "# question_schemas = [\"What are the names of all the employees in the HR department?|CREATE TABLE Employees (id INT, name TEXT, department TEXT)\"]\n",
        "with open('serialized_real_data_str.json', 'r') as file:\n",
        "    question_schemas = json.load(file)\n",
        "\n",
        "instruction = \"You are a semantic parser to translate natural language question to SQL query. In each question, the database schema will be provided. In the SQL you don't need to indicate the name of the database, just write SQL within the database. Please give only the SQL queries without any text explanations\"\n",
        "example = \"Here is an example including input and output. Input: question: How many heads of the departments are older than 56 ? serialized_schema:  | department_management | department : department_id , name , creation , ranking , budget_in_billions , num_employees | head : head_id , name , born_state , age | management : department_id , head_id , temporary_acting. Output: select count(*) from head where age > 56\"\n",
        "#messages = [{\"role\": \"system\", \"content\": instruction}]\n",
        "results,predictions = [],[]\n",
        "\n",
        "for question_schema in question_schemas: \n",
        "    messages = [{\"role\": \"system\", \"content\": instruction}]\n",
        "    #messages = [{\"role\": \"system\", \"content\": instruction},{\"role\": \"system\", \"content\": example}]   \n",
        "    messages.append({\"role\": \"user\", \"content\": question_schema})\n",
        "    completion = openai.Completion.create(\n",
        "        engine=\"text-davinci-003\",\n",
        "        prompt=\"\".join([m[\"content\"] for m in messages]),\n",
        "        max_tokens=200,\n",
        "        temperature=0,\n",
        "        n = 1,\n",
        "        stop=None,\n",
        "        timeout=15,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "    chat_response = completion.choices[0].text.strip()\n",
        "    messages.append({\"role\": \"assistant\", \"content\": chat_response}) \n",
        "    sql_query = chat_response.replace(\"\\n\", \" \")\n",
        "    print(sql_query)\n",
        "\n",
        "    results.append({\"input\": question_schema, \"sql_query\": sql_query})\n",
        "    predictions.append(sql_query)\n",
        "\n",
        "# for i in range(1, len(messages)-1, 2):\n",
        "#     question = messages[i][\"content\"]\n",
        "#     sql_query = messages[i+1][\"content\"].replace(\"\\n\", \"\")\n",
        "#     results.append({\"input\": question, \"sql_query\": sql_query})\n",
        "\n",
        "# with open(\"results_text_davinci_003.json\", \"w\") as file:\n",
        "#     json.dump(results, file, indent=4)\n",
        "\n",
        "with open(\"predictions_text_davinci_003_real.json\", \"w\") as file:\n",
        "    json.dump(predictions, file, indent=4)\n",
        "\n",
        "files.download(\"predictions_text_davinci_003_real.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjl5nUYg4u2Q",
        "outputId": "275378c1-373a-48cb-879c-f268027f06f9"
      },
      "outputs": [],
      "source": [
        "print(len(predictions))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EkkFQtxuSePD"
      },
      "source": [
        "### Dr.Spider"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mH2FnFzj0yKw"
      },
      "source": [
        "#### DB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCDClmdnSixv"
      },
      "outputs": [],
      "source": [
        "### Dr.Spider, db_content_equivalence, gpt-3.5-turbo\n",
        "\n",
        "import openai\n",
        "import json\n",
        "from google.colab import files\n",
        "# question_schemas = [\"What are the names of all the employees in the HR department?|CREATE TABLE Employees (id INT, name TEXT, department TEXT)\"]\n",
        "with open('serialized_dr_db_content_equivalence_data_str.json', 'r') as file:\n",
        "    question_schemas = json.load(file)\n",
        "\n",
        "instruction = \"You are a semantic parser to translate natural language question to SQL query. In each question, the database schema will be provided. In the SQL you don't need to indicate the name of the database, just write SQL within the database. Please give only the SQL queries without any text explanations\"\n",
        "example = \"Here is an example including input and output. Input: question: How many heads of the departments are older than 56 ? serialized_schema:  | department_management | department : department_id , name , creation , ranking , budget_in_billions , num_employees | head : head_id , name , born_state , age | management : department_id , head_id , temporary_acting. Output: select count(*) from head where age > 56\"\n",
        "example_2 = \"Here is another example including input and output. Input: question: List the name, born state and age of the heads of departments ordered by age. serialized_schema:  | department_management | department : department_id , name , creation , ranking , budget_in_billions , num_employees | head : head_id , name , born_state , age | management : department_id , head_id , temporary_acting. Output: select name, born_state, age from head order by age\"\n",
        "\n",
        "#messages = [{\"role\": \"system\", \"content\": instruction}]\n",
        "\n",
        "results,predictions = [],[]\n",
        "\n",
        "for question_schema in question_schemas: \n",
        "    #messages = [{\"role\": \"system\", \"content\": instruction}]\n",
        "    messages = [{\"role\": \"system\", \"content\": instruction},{\"role\": \"system\", \"content\": example}] \n",
        "    #messages = [{\"role\": \"system\", \"content\": instruction},{\"role\": \"system\", \"content\": example},{\"role\": \"system\", \"content\": example_2}] \n",
        "    messages.append({\"role\": \"user\", \"content\": question_schema})\n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        max_tokens=200,\n",
        "        temperature=0,\n",
        "        n = 1,\n",
        "        stop=None,\n",
        "        timeout=15,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "    chat_response= completion.choices[0].message.content\n",
        "    sql_query=chat_response.replace(\"\\n\", \" \")\n",
        "\n",
        "    print(sql_query)\n",
        "    messages.append({\"role\": \"assistant\", \"content\": chat_response}) \n",
        "    results.append({\"input\": question_schema, \"sql_query\": sql_query})\n",
        "    predictions.append(sql_query)\n",
        "\n",
        "# with open(\"results_gpt_3.5_turbo.json\", \"w\") as file:\n",
        "#     json.dump(results, file, indent=4)\n",
        "with open(\"predictions_gpt_3.5_turbo_db_content_equivalence.json\", \"w\") as file:\n",
        "    json.dump(predictions, file, indent=4)\n",
        "files.download(\"predictions_gpt_3.5_turbo_db_content_equivalence.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WwM23FZdGOb"
      },
      "outputs": [],
      "source": [
        "### Dr.Spider, db_schema_abbreviation, gpt-3.5-turbo\n",
        "\n",
        "import openai\n",
        "import json\n",
        "from google.colab import files\n",
        "# question_schemas = [\"What are the names of all the employees in the HR department?|CREATE TABLE Employees (id INT, name TEXT, department TEXT)\"]\n",
        "with open('serialized_dr_db_schema_abbreviation_data_str.json', 'r') as file:\n",
        "    question_schemas = json.load(file)\n",
        "\n",
        "instruction = \"You are a semantic parser to translate natural language question to SQL query. In each question, the database schema will be provided. In the SQL you don't need to indicate the name of the database, just write SQL within the database. Please give only the SQL queries without any text explanations\"\n",
        "example = \"Here is an example including input and output. Input: question: How many heads of the departments are older than 56 ? serialized_schema:  | department_management | department : department_id , name , creation , ranking , budget_in_billions , num_employees | head : head_id , name , born_state , age | management : department_id , head_id , temporary_acting. Output: select count(*) from head where age > 56\"\n",
        "example_2 = \"Here is another example including input and output. Input: question: List the name, born state and age of the heads of departments ordered by age. serialized_schema:  | department_management | department : department_id , name , creation , ranking , budget_in_billions , num_employees | head : head_id , name , born_state , age | management : department_id , head_id , temporary_acting. Output: select name, born_state, age from head order by age\"\n",
        "\n",
        "#messages = [{\"role\": \"system\", \"content\": instruction}]\n",
        "\n",
        "results,predictions = [],[]\n",
        "\n",
        "for question_schema in question_schemas: \n",
        "    #messages = [{\"role\": \"system\", \"content\": instruction}]\n",
        "    messages = [{\"role\": \"system\", \"content\": instruction},{\"role\": \"system\", \"content\": example}] \n",
        "    #messages = [{\"role\": \"system\", \"content\": instruction},{\"role\": \"system\", \"content\": example},{\"role\": \"system\", \"content\": example_2}] \n",
        "    messages.append({\"role\": \"user\", \"content\": question_schema})\n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        max_tokens=200,\n",
        "        temperature=0,\n",
        "        n = 1,\n",
        "        stop=None,\n",
        "        timeout=15,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "    chat_response= completion.choices[0].message.content\n",
        "    sql_query=chat_response.replace(\"\\n\", \" \")\n",
        "\n",
        "    print(sql_query)\n",
        "    messages.append({\"role\": \"assistant\", \"content\": chat_response}) \n",
        "    results.append({\"input\": question_schema, \"sql_query\": sql_query})\n",
        "    predictions.append(sql_query)\n",
        "\n",
        "# with open(\"results_gpt_3.5_turbo.json\", \"w\") as file:\n",
        "#     json.dump(results, file, indent=4)\n",
        "with open(\"predictions_gpt_3.5_turbo_db_schema_abbreviation.json\", \"w\") as file:\n",
        "    json.dump(predictions, file, indent=4)\n",
        "files.download(\"predictions_gpt_3.5_turbo_db_schema_abbreviation.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7dWknspz8PW"
      },
      "outputs": [],
      "source": [
        "### Dr.Spider, db_schema_synonym, gpt-3.5-turbo\n",
        "\n",
        "import openai\n",
        "import json\n",
        "from google.colab import files\n",
        "# question_schemas = [\"What are the names of all the employees in the HR department?|CREATE TABLE Employees (id INT, name TEXT, department TEXT)\"]\n",
        "with open('serialized_dr_db_schema_synonym_data_str.json', 'r') as file:\n",
        "    question_schemas = json.load(file)\n",
        "\n",
        "instruction = \"You are a semantic parser to translate natural language question to SQL query. In each question, the database schema will be provided. In the SQL you don't need to indicate the name of the database, just write SQL within the database. Please give only the SQL queries without any text explanations\"\n",
        "example = \"Here is an example including input and output. Input: question: How many heads of the departments are older than 56 ? serialized_schema:  | department_management | department : department_id , name , creation , ranking , budget_in_billions , num_employees | head : head_id , name , born_state , age | management : department_id , head_id , temporary_acting. Output: select count(*) from head where age > 56\"\n",
        "example_2 = \"Here is another example including input and output. Input: question: List the name, born state and age of the heads of departments ordered by age. serialized_schema:  | department_management | department : department_id , name , creation , ranking , budget_in_billions , num_employees | head : head_id , name , born_state , age | management : department_id , head_id , temporary_acting. Output: select name, born_state, age from head order by age\"\n",
        "\n",
        "#messages = [{\"role\": \"system\", \"content\": instruction}]\n",
        "\n",
        "results,predictions = [],[]\n",
        "\n",
        "for question_schema in question_schemas: \n",
        "    #messages = [{\"role\": \"system\", \"content\": instruction}]\n",
        "    messages = [{\"role\": \"system\", \"content\": instruction},{\"role\": \"system\", \"content\": example}] \n",
        "    #messages = [{\"role\": \"system\", \"content\": instruction},{\"role\": \"system\", \"content\": example},{\"role\": \"system\", \"content\": example_2}] \n",
        "    messages.append({\"role\": \"user\", \"content\": question_schema})\n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        max_tokens=200,\n",
        "        temperature=0,\n",
        "        n = 1,\n",
        "        stop=None,\n",
        "        timeout=15,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "    chat_response= completion.choices[0].message.content\n",
        "    sql_query=chat_response.replace(\"\\n\", \" \")\n",
        "\n",
        "    print(sql_query)\n",
        "    messages.append({\"role\": \"assistant\", \"content\": chat_response}) \n",
        "    results.append({\"input\": question_schema, \"sql_query\": sql_query})\n",
        "    predictions.append(sql_query)\n",
        "\n",
        "# with open(\"results_gpt_3.5_turbo.json\", \"w\") as file:\n",
        "#     json.dump(results, file, indent=4)\n",
        "with open(\"predictions_gpt_3.5_turbo_db_schema_synonym.json\", \"w\") as file:\n",
        "    json.dump(predictions, file, indent=4)\n",
        "files.download(\"predictions_gpt_3.5_turbo_db_schema_synonym.json\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "H_AaRLX26qBD"
      },
      "source": [
        "#### NLQ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FknCwLA66sL_"
      },
      "outputs": [],
      "source": [
        "### Dr.Spider, nlq_column_attribute, gpt-3.5-turbo\n",
        "\n",
        "import openai\n",
        "import json\n",
        "from google.colab import files\n",
        "# question_schemas = [\"What are the names of all the employees in the HR department?|CREATE TABLE Employees (id INT, name TEXT, department TEXT)\"]\n",
        "with open('serialized_dr_nlq_column_attribute_data_str.json', 'r') as file:\n",
        "    question_schemas = json.load(file)\n",
        "\n",
        "instruction = \"You are a semantic parser to translate natural language question to SQL query. In each question, the database schema will be provided. In the SQL you don't need to indicate the name of the database, just write SQL within the database. Please give only the SQL queries without any text explanations\"\n",
        "example = \"Here is an example including input and output. Input: question: How many heads of the departments are older than 56 ? serialized_schema:  | department_management | department : department_id , name , creation , ranking , budget_in_billions , num_employees | head : head_id , name , born_state , age | management : department_id , head_id , temporary_acting. Output: select count(*) from head where age > 56\"\n",
        "example_2 = \"Here is another example including input and output. Input: question: List the name, born state and age of the heads of departments ordered by age. serialized_schema:  | department_management | department : department_id , name , creation , ranking , budget_in_billions , num_employees | head : head_id , name , born_state , age | management : department_id , head_id , temporary_acting. Output: select name, born_state, age from head order by age\"\n",
        "\n",
        "#messages = [{\"role\": \"system\", \"content\": instruction}]\n",
        "\n",
        "results,predictions = [],[]\n",
        "\n",
        "for question_schema in question_schemas: \n",
        "    #messages = [{\"role\": \"system\", \"content\": instruction}]\n",
        "    messages = [{\"role\": \"system\", \"content\": instruction},{\"role\": \"system\", \"content\": example}] \n",
        "    #messages = [{\"role\": \"system\", \"content\": instruction},{\"role\": \"system\", \"content\": example},{\"role\": \"system\", \"content\": example_2}] \n",
        "    messages.append({\"role\": \"user\", \"content\": question_schema})\n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        max_tokens=200,\n",
        "        temperature=0,\n",
        "        n = 1,\n",
        "        stop=None,\n",
        "        timeout=15,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "    chat_response= completion.choices[0].message.content\n",
        "    sql_query=chat_response.replace(\"\\n\", \" \")\n",
        "\n",
        "    print(sql_query)\n",
        "    messages.append({\"role\": \"assistant\", \"content\": chat_response}) \n",
        "    results.append({\"input\": question_schema, \"sql_query\": sql_query})\n",
        "    predictions.append(sql_query)\n",
        "\n",
        "# with open(\"results_gpt_3.5_turbo.json\", \"w\") as file:\n",
        "#     json.dump(results, file, indent=4)\n",
        "with open(\"predictions_gpt_3.5_turbo_nlq_column_attribute.json\", \"w\") as file:\n",
        "    json.dump(predictions, file, indent=4)\n",
        "files.download(\"predictions_gpt_3.5_turbo_nlq_column_attribute.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lM2jWjNH_c8k"
      },
      "outputs": [],
      "source": [
        "### Dr.Spider, nlq_column_carrier, gpt-3.5-turbo\n",
        "\n",
        "import openai\n",
        "import json\n",
        "from google.colab import files\n",
        "# question_schemas = [\"What are the names of all the employees in the HR department?|CREATE TABLE Employees (id INT, name TEXT, department TEXT)\"]\n",
        "with open('serialized_dr_nlq_column_carrier_data_str.json', 'r') as file:\n",
        "    question_schemas = json.load(file)\n",
        "\n",
        "instruction = \"You are a semantic parser to translate natural language question to SQL query. In each question, the database schema will be provided. In the SQL you don't need to indicate the name of the database, just write SQL within the database. Please give only the SQL queries without any text explanations\"\n",
        "example = \"Here is an example including input and output. Input: question: How many heads of the departments are older than 56 ? serialized_schema:  | department_management | department : department_id , name , creation , ranking , budget_in_billions , num_employees | head : head_id , name , born_state , age | management : department_id , head_id , temporary_acting. Output: select count(*) from head where age > 56\"\n",
        "example_2 = \"Here is another example including input and output. Input: question: List the name, born state and age of the heads of departments ordered by age. serialized_schema:  | department_management | department : department_id , name , creation , ranking , budget_in_billions , num_employees | head : head_id , name , born_state , age | management : department_id , head_id , temporary_acting. Output: select name, born_state, age from head order by age\"\n",
        "\n",
        "#messages = [{\"role\": \"system\", \"content\": instruction}]\n",
        "\n",
        "results,predictions = [],[]\n",
        "\n",
        "for question_schema in question_schemas: \n",
        "    #messages = [{\"role\": \"system\", \"content\": instruction}]\n",
        "    messages = [{\"role\": \"system\", \"content\": instruction},{\"role\": \"system\", \"content\": example}] \n",
        "    #messages = [{\"role\": \"system\", \"content\": instruction},{\"role\": \"system\", \"content\": example},{\"role\": \"system\", \"content\": example_2}] \n",
        "    messages.append({\"role\": \"user\", \"content\": question_schema})\n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        max_tokens=200,\n",
        "        temperature=0,\n",
        "        n = 1,\n",
        "        stop=None,\n",
        "        timeout=15,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "    chat_response= completion.choices[0].message.content\n",
        "    sql_query=chat_response.replace(\"\\n\", \" \")\n",
        "\n",
        "    print(sql_query)\n",
        "    messages.append({\"role\": \"assistant\", \"content\": chat_response}) \n",
        "    results.append({\"input\": question_schema, \"sql_query\": sql_query})\n",
        "    predictions.append(sql_query)\n",
        "\n",
        "# with open(\"results_gpt_3.5_turbo.json\", \"w\") as file:\n",
        "#     json.dump(results, file, indent=4)\n",
        "with open(\"predictions_gpt_3.5_turbo_nlq_column_carrier.json\", \"w\") as file:\n",
        "    json.dump(predictions, file, indent=4)\n",
        "files.download(\"predictions_gpt_3.5_turbo_nlq_column_carrier.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQ2_xpoyDbq7"
      },
      "outputs": [],
      "source": [
        "### Dr.Spider, nlq_column_synonym, gpt-3.5-turbo\n",
        "\n",
        "import openai\n",
        "import json\n",
        "from google.colab import files\n",
        "# question_schemas = [\"What are the names of all the employees in the HR department?|CREATE TABLE Employees (id INT, name TEXT, department TEXT)\"]\n",
        "with open('serialized_dr_nlq_column_synonym_data_str.json', 'r') as file:\n",
        "    question_schemas = json.load(file)\n",
        "\n",
        "instruction = \"You are a semantic parser to translate natural language question to SQL query. In each question, the database schema will be provided. In the SQL you don't need to indicate the name of the database, just write SQL within the database. Please give only the SQL queries without any text explanations\"\n",
        "example = \"Here is an example including input and output. Input: question: How many heads of the departments are older than 56 ? serialized_schema:  | department_management | department : department_id , name , creation , ranking , budget_in_billions , num_employees | head : head_id , name , born_state , age | management : department_id , head_id , temporary_acting. Output: select count(*) from head where age > 56\"\n",
        "example_2 = \"Here is another example including input and output. Input: question: List the name, born state and age of the heads of departments ordered by age. serialized_schema:  | department_management | department : department_id , name , creation , ranking , budget_in_billions , num_employees | head : head_id , name , born_state , age | management : department_id , head_id , temporary_acting. Output: select name, born_state, age from head order by age\"\n",
        "\n",
        "#messages = [{\"role\": \"system\", \"content\": instruction}]\n",
        "\n",
        "results,predictions = [],[]\n",
        "\n",
        "for question_schema in question_schemas: \n",
        "    #messages = [{\"role\": \"system\", \"content\": instruction}]\n",
        "    messages = [{\"role\": \"system\", \"content\": instruction},{\"role\": \"system\", \"content\": example}] \n",
        "    #messages = [{\"role\": \"system\", \"content\": instruction},{\"role\": \"system\", \"content\": example},{\"role\": \"system\", \"content\": example_2}] \n",
        "    messages.append({\"role\": \"user\", \"content\": question_schema})\n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        max_tokens=200,\n",
        "        temperature=0,\n",
        "        n = 1,\n",
        "        stop=None,\n",
        "        timeout=15,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "    chat_response= completion.choices[0].message.content\n",
        "    sql_query=chat_response.replace(\"\\n\", \" \")\n",
        "\n",
        "    print(sql_query)\n",
        "    messages.append({\"role\": \"assistant\", \"content\": chat_response}) \n",
        "    results.append({\"input\": question_schema, \"sql_query\": sql_query})\n",
        "    predictions.append(sql_query)\n",
        "\n",
        "# with open(\"results_gpt_3.5_turbo.json\", \"w\") as file:\n",
        "#     json.dump(results, file, indent=4)\n",
        "with open(\"predictions_gpt_3.5_turbo_nlq_column_synonym.json\", \"w\") as file:\n",
        "    json.dump(predictions, file, indent=4)\n",
        "files.download(\"predictions_gpt_3.5_turbo_nlq_column_synonym.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9rYstUhG4nM"
      },
      "outputs": [],
      "source": [
        "### Dr.Spider, nlq_column_value, gpt-3.5-turbo\n",
        "\n",
        "import openai\n",
        "import json\n",
        "from google.colab import files\n",
        "# question_schemas = [\"What are the names of all the employees in the HR department?|CREATE TABLE Employees (id INT, name TEXT, department TEXT)\"]\n",
        "with open('serialized_dr_nlq_column_value_data_str.json', 'r') as file:\n",
        "    question_schemas = json.load(file)\n",
        "\n",
        "instruction = \"You are a semantic parser to translate natural language question to SQL query. In each question, the database schema will be provided. In the SQL you don't need to indicate the name of the database, just write SQL within the database. Please give only the SQL queries without any text explanations\"\n",
        "example = \"Here is an example including input and output. Input: question: How many heads of the departments are older than 56 ? serialized_schema:  | department_management | department : department_id , name , creation , ranking , budget_in_billions , num_employees | head : head_id , name , born_state , age | management : department_id , head_id , temporary_acting. Output: select count(*) from head where age > 56\"\n",
        "example_2 = \"Here is another example including input and output. Input: question: List the name, born state and age of the heads of departments ordered by age. serialized_schema:  | department_management | department : department_id , name , creation , ranking , budget_in_billions , num_employees | head : head_id , name , born_state , age | management : department_id , head_id , temporary_acting. Output: select name, born_state, age from head order by age\"\n",
        "\n",
        "#messages = [{\"role\": \"system\", \"content\": instruction}]\n",
        "\n",
        "results,predictions = [],[]\n",
        "\n",
        "for question_schema in question_schemas: \n",
        "    #messages = [{\"role\": \"system\", \"content\": instruction}]\n",
        "    messages = [{\"role\": \"system\", \"content\": instruction},{\"role\": \"system\", \"content\": example}] \n",
        "    #messages = [{\"role\": \"system\", \"content\": instruction},{\"role\": \"system\", \"content\": example},{\"role\": \"system\", \"content\": example_2}] \n",
        "    messages.append({\"role\": \"user\", \"content\": question_schema})\n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        max_tokens=200,\n",
        "        temperature=0,\n",
        "        n = 1,\n",
        "        stop=None,\n",
        "        timeout=15,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "    chat_response= completion.choices[0].message.content\n",
        "    sql_query=chat_response.replace(\"\\n\", \" \")\n",
        "\n",
        "    print(sql_query)\n",
        "    messages.append({\"role\": \"assistant\", \"content\": chat_response}) \n",
        "    results.append({\"input\": question_schema, \"sql_query\": sql_query})\n",
        "    predictions.append(sql_query)\n",
        "\n",
        "# with open(\"results_gpt_3.5_turbo.json\", \"w\") as file:\n",
        "#     json.dump(results, file, indent=4)\n",
        "with open(\"predictions_gpt_3.5_turbo_nlq_column_value.json\", \"w\") as file:\n",
        "    json.dump(predictions, file, indent=4)\n",
        "files.download(\"predictions_gpt_3.5_turbo_nlq_column_value.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9kQmGaQSNNd"
      },
      "outputs": [],
      "source": [
        "### Dr.Spider, nlq_keyword_carrier, gpt-3.5-turbo\n",
        "\n",
        "import openai\n",
        "import json\n",
        "from google.colab import files\n",
        "# question_schemas = [\"What are the names of all the employees in the HR department?|CREATE TABLE Employees (id INT, name TEXT, department TEXT)\"]\n",
        "with open('serialized_dr_nlq_keyword_carrier_data_str.json', 'r') as file:\n",
        "    question_schemas = json.load(file)\n",
        "\n",
        "instruction = \"You are a semantic parser to translate natural language question to SQL query. In each question, the database schema will be provided. In the SQL you don't need to indicate the name of the database, just write SQL within the database. Please give only the SQL queries without any text explanations\"\n",
        "example = \"Here is an example including input and output. Input: question: How many heads of the departments are older than 56 ? serialized_schema:  | department_management | department : department_id , name , creation , ranking , budget_in_billions , num_employees | head : head_id , name , born_state , age | management : department_id , head_id , temporary_acting. Output: select count(*) from head where age > 56\"\n",
        "example_2 = \"Here is another example including input and output. Input: question: List the name, born state and age of the heads of departments ordered by age. serialized_schema:  | department_management | department : department_id , name , creation , ranking , budget_in_billions , num_employees | head : head_id , name , born_state , age | management : department_id , head_id , temporary_acting. Output: select name, born_state, age from head order by age\"\n",
        "\n",
        "#messages = [{\"role\": \"system\", \"content\": instruction}]\n",
        "\n",
        "results,predictions = [],[]\n",
        "\n",
        "for question_schema in question_schemas: \n",
        "    #messages = [{\"role\": \"system\", \"content\": instruction}]\n",
        "    messages = [{\"role\": \"system\", \"content\": instruction},{\"role\": \"system\", \"content\": example}] \n",
        "    #messages = [{\"role\": \"system\", \"content\": instruction},{\"role\": \"system\", \"content\": example},{\"role\": \"system\", \"content\": example_2}] \n",
        "    messages.append({\"role\": \"user\", \"content\": question_schema})\n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        max_tokens=200,\n",
        "        temperature=0,\n",
        "        n = 1,\n",
        "        stop=None,\n",
        "        timeout=15,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "    chat_response= completion.choices[0].message.content\n",
        "    sql_query=chat_response.replace(\"\\n\", \" \")\n",
        "\n",
        "    print(sql_query)\n",
        "    messages.append({\"role\": \"assistant\", \"content\": chat_response}) \n",
        "    results.append({\"input\": question_schema, \"sql_query\": sql_query})\n",
        "    predictions.append(sql_query)\n",
        "\n",
        "# with open(\"results_gpt_3.5_turbo.json\", \"w\") as file:\n",
        "#     json.dump(results, file, indent=4)\n",
        "with open(\"predictions_gpt_3.5_turbo_nlq_keyword_carrier.json\", \"w\") as file:\n",
        "    json.dump(predictions, file, indent=4)\n",
        "files.download(\"predictions_gpt_3.5_turbo_nlq_keyword_carrier.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-lZ6M9uJMd1"
      },
      "outputs": [],
      "source": [
        "### Dr.Spider, nlq_keyword_synonym, gpt-3.5-turbo\n",
        "\n",
        "import openai\n",
        "import json\n",
        "from google.colab import files\n",
        "# question_schemas = [\"What are the names of all the employees in the HR department?|CREATE TABLE Employees (id INT, name TEXT, department TEXT)\"]\n",
        "with open('serialized_dr_nlq_keyword_synonym_data_str.json', 'r') as file:\n",
        "    question_schemas = json.load(file)\n",
        "\n",
        "instruction = \"You are a semantic parser to translate natural language question to SQL query. In each question, the database schema will be provided. In the SQL you don't need to indicate the name of the database, just write SQL within the database. Please give only the SQL queries without any text explanations\"\n",
        "example = \"Here is an example including input and output. Input: question: How many heads of the departments are older than 56 ? serialized_schema:  | department_management | department : department_id , name , creation , ranking , budget_in_billions , num_employees | head : head_id , name , born_state , age | management : department_id , head_id , temporary_acting. Output: select count(*) from head where age > 56\"\n",
        "example_2 = \"Here is another example including input and output. Input: question: List the name, born state and age of the heads of departments ordered by age. serialized_schema:  | department_management | department : department_id , name , creation , ranking , budget_in_billions , num_employees | head : head_id , name , born_state , age | management : department_id , head_id , temporary_acting. Output: select name, born_state, age from head order by age\"\n",
        "\n",
        "#messages = [{\"role\": \"system\", \"content\": instruction}]\n",
        "\n",
        "results,predictions = [],[]\n",
        "\n",
        "for question_schema in question_schemas: \n",
        "    #messages = [{\"role\": \"system\", \"content\": instruction}]\n",
        "    messages = [{\"role\": \"system\", \"content\": instruction},{\"role\": \"system\", \"content\": example}] \n",
        "    #messages = [{\"role\": \"system\", \"content\": instruction},{\"role\": \"system\", \"content\": example},{\"role\": \"system\", \"content\": example_2}] \n",
        "    messages.append({\"role\": \"user\", \"content\": question_schema})\n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        max_tokens=200,\n",
        "        temperature=0,\n",
        "        n = 1,\n",
        "        stop=None,\n",
        "        timeout=15,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "    chat_response= completion.choices[0].message.content\n",
        "    sql_query=chat_response.replace(\"\\n\", \" \")\n",
        "\n",
        "    print(sql_query)\n",
        "    messages.append({\"role\": \"assistant\", \"content\": chat_response}) \n",
        "    results.append({\"input\": question_schema, \"sql_query\": sql_query})\n",
        "    predictions.append(sql_query)\n",
        "\n",
        "# with open(\"results_gpt_3.5_turbo.json\", \"w\") as file:\n",
        "#     json.dump(results, file, indent=4)\n",
        "with open(\"predictions_gpt_3.5_turbo_nlq_keyword_synonym.json\", \"w\") as file:\n",
        "    json.dump(predictions, file, indent=4)\n",
        "files.download(\"predictions_gpt_3.5_turbo_nlq_keyword_synonym.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HG_iJVfQSUnu"
      },
      "outputs": [],
      "source": [
        "### Dr.Spider, nlq_multitype, gpt-3.5-turbo\n",
        "\n",
        "import openai\n",
        "import json\n",
        "from google.colab import files\n",
        "# question_schemas = [\"What are the names of all the employees in the HR department?|CREATE TABLE Employees (id INT, name TEXT, department TEXT)\"]\n",
        "with open('serialized_dr_nlq_multitype_data_str.json', 'r') as file:\n",
        "    question_schemas = json.load(file)\n",
        "\n",
        "instruction = \"You are a semantic parser to translate natural language question to SQL query. In each question, the database schema will be provided. In the SQL you don't need to indicate the name of the database, just write SQL within the database. Please give only the SQL queries without any text explanations\"\n",
        "example = \"Here is an example including input and output. Input: question: How many heads of the departments are older than 56 ? serialized_schema:  | department_management | department : department_id , name , creation , ranking , budget_in_billions , num_employees | head : head_id , name , born_state , age | management : department_id , head_id , temporary_acting. Output: select count(*) from head where age > 56\"\n",
        "example_2 = \"Here is another example including input and output. Input: question: List the name, born state and age of the heads of departments ordered by age. serialized_schema:  | department_management | department : department_id , name , creation , ranking , budget_in_billions , num_employees | head : head_id , name , born_state , age | management : department_id , head_id , temporary_acting. Output: select name, born_state, age from head order by age\"\n",
        "\n",
        "#messages = [{\"role\": \"system\", \"content\": instruction}]\n",
        "\n",
        "results,predictions = [],[]\n",
        "\n",
        "for question_schema in question_schemas: \n",
        "    #messages = [{\"role\": \"system\", \"content\": instruction}]\n",
        "    messages = [{\"role\": \"system\", \"content\": instruction},{\"role\": \"system\", \"content\": example}] \n",
        "    #messages = [{\"role\": \"system\", \"content\": instruction},{\"role\": \"system\", \"content\": example},{\"role\": \"system\", \"content\": example_2}] \n",
        "    messages.append({\"role\": \"user\", \"content\": question_schema})\n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        max_tokens=200,\n",
        "        temperature=0,\n",
        "        n = 1,\n",
        "        stop=None,\n",
        "        timeout=15,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "    chat_response= completion.choices[0].message.content\n",
        "    sql_query=chat_response.replace(\"\\n\", \" \")\n",
        "\n",
        "    print(sql_query)\n",
        "    messages.append({\"role\": \"assistant\", \"content\": chat_response}) \n",
        "    results.append({\"input\": question_schema, \"sql_query\": sql_query})\n",
        "    predictions.append(sql_query)\n",
        "\n",
        "# with open(\"results_gpt_3.5_turbo.json\", \"w\") as file:\n",
        "#     json.dump(results, file, indent=4)\n",
        "with open(\"predictions_gpt_3.5_turbo_nlq_multitype.json\", \"w\") as file:\n",
        "    json.dump(predictions, file, indent=4)\n",
        "files.download(\"predictions_gpt_3.5_turbo_nlq_multitype.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWZcpsxCSaNM"
      },
      "outputs": [],
      "source": [
        "### Dr.Spider, nlq_others, gpt-3.5-turbo\n",
        "\n",
        "import openai\n",
        "import json\n",
        "from google.colab import files\n",
        "# question_schemas = [\"What are the names of all the employees in the HR department?|CREATE TABLE Employees (id INT, name TEXT, department TEXT)\"]\n",
        "with open('serialized_dr_nlq_others_data_str.json', 'r') as file:\n",
        "    question_schemas = json.load(file)\n",
        "\n",
        "instruction = \"You are a semantic parser to translate natural language question to SQL query. In each question, the database schema will be provided. In the SQL you don't need to indicate the name of the database, just write SQL within the database. Please give only the SQL queries without any text explanations\"\n",
        "example = \"Here is an example including input and output. Input: question: How many heads of the departments are older than 56 ? serialized_schema:  | department_management | department : department_id , name , creation , ranking , budget_in_billions , num_employees | head : head_id , name , born_state , age | management : department_id , head_id , temporary_acting. Output: select count(*) from head where age > 56\"\n",
        "example_2 = \"Here is another example including input and output. Input: question: List the name, born state and age of the heads of departments ordered by age. serialized_schema:  | department_management | department : department_id , name , creation , ranking , budget_in_billions , num_employees | head : head_id , name , born_state , age | management : department_id , head_id , temporary_acting. Output: select name, born_state, age from head order by age\"\n",
        "\n",
        "#messages = [{\"role\": \"system\", \"content\": instruction}]\n",
        "\n",
        "results,predictions = [],[]\n",
        "\n",
        "for question_schema in question_schemas: \n",
        "    #messages = [{\"role\": \"system\", \"content\": instruction}]\n",
        "    messages = [{\"role\": \"system\", \"content\": instruction},{\"role\": \"system\", \"content\": example}] \n",
        "    #messages = [{\"role\": \"system\", \"content\": instruction},{\"role\": \"system\", \"content\": example},{\"role\": \"system\", \"content\": example_2}] \n",
        "    messages.append({\"role\": \"user\", \"content\": question_schema})\n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        max_tokens=200,\n",
        "        temperature=0,\n",
        "        n = 1,\n",
        "        stop=None,\n",
        "        timeout=15,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "    chat_response= completion.choices[0].message.content\n",
        "    sql_query=chat_response.replace(\"\\n\", \" \")\n",
        "\n",
        "    print(sql_query)\n",
        "    messages.append({\"role\": \"assistant\", \"content\": chat_response}) \n",
        "    results.append({\"input\": question_schema, \"sql_query\": sql_query})\n",
        "    predictions.append(sql_query)\n",
        "\n",
        "# with open(\"results_gpt_3.5_turbo.json\", \"w\") as file:\n",
        "#     json.dump(results, file, indent=4)\n",
        "with open(\"predictions_gpt_3.5_turbo_nlq_others.json\", \"w\") as file:\n",
        "    json.dump(predictions, file, indent=4)\n",
        "files.download(\"predictions_gpt_3.5_turbo_nlq_others.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SeyPG9--ScNy"
      },
      "outputs": [],
      "source": [
        "### Dr.Spider, nlq_value_synonym, gpt-3.5-turbo\n",
        "\n",
        "import openai\n",
        "import json\n",
        "from google.colab import files\n",
        "# question_schemas = [\"What are the names of all the employees in the HR department?|CREATE TABLE Employees (id INT, name TEXT, department TEXT)\"]\n",
        "with open('serialized_dr_nlq_value_synonym_data_str.json', 'r') as file:\n",
        "    question_schemas = json.load(file)\n",
        "\n",
        "instruction = \"You are a semantic parser to translate natural language question to SQL query. In each question, the database schema will be provided. In the SQL you don't need to indicate the name of the database, just write SQL within the database. Please give only the SQL queries without any text explanations\"\n",
        "example = \"Here is an example including input and output. Input: question: How many heads of the departments are older than 56 ? serialized_schema:  | department_management | department : department_id , name , creation , ranking , budget_in_billions , num_employees | head : head_id , name , born_state , age | management : department_id , head_id , temporary_acting. Output: select count(*) from head where age > 56\"\n",
        "example_2 = \"Here is another example including input and output. Input: question: List the name, born state and age of the heads of departments ordered by age. serialized_schema:  | department_management | department : department_id , name , creation , ranking , budget_in_billions , num_employees | head : head_id , name , born_state , age | management : department_id , head_id , temporary_acting. Output: select name, born_state, age from head order by age\"\n",
        "\n",
        "#messages = [{\"role\": \"system\", \"content\": instruction}]\n",
        "\n",
        "results,predictions = [],[]\n",
        "\n",
        "for question_schema in question_schemas: \n",
        "    #messages = [{\"role\": \"system\", \"content\": instruction}]\n",
        "    messages = [{\"role\": \"system\", \"content\": instruction},{\"role\": \"system\", \"content\": example}] \n",
        "    #messages = [{\"role\": \"system\", \"content\": instruction},{\"role\": \"system\", \"content\": example},{\"role\": \"system\", \"content\": example_2}] \n",
        "    messages.append({\"role\": \"user\", \"content\": question_schema})\n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        max_tokens=200,\n",
        "        temperature=0,\n",
        "        n = 1,\n",
        "        stop=None,\n",
        "        timeout=15,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "    chat_response= completion.choices[0].message.content\n",
        "    sql_query=chat_response.replace(\"\\n\", \" \")\n",
        "\n",
        "    print(sql_query)\n",
        "    messages.append({\"role\": \"assistant\", \"content\": chat_response}) \n",
        "    results.append({\"input\": question_schema, \"sql_query\": sql_query})\n",
        "    predictions.append(sql_query)\n",
        "\n",
        "# with open(\"results_gpt_3.5_turbo.json\", \"w\") as file:\n",
        "#     json.dump(results, file, indent=4)\n",
        "with open(\"predictions_gpt_3.5_turbo_nlq_value_synonym.json\", \"w\") as file:\n",
        "    json.dump(predictions, file, indent=4)\n",
        "files.download(\"predictions_gpt_3.5_turbo_nlq_value_synonym.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deeKJsphgIrt",
        "outputId": "189f26a0-621f-499e-bc3b-7b4335757c73"
      },
      "outputs": [],
      "source": [
        "print(len(predictions))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6ppNyciI3lX5"
      },
      "source": [
        "#### SQL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ej7-kh7o3jz6"
      },
      "outputs": [],
      "source": [
        "### Dr.Spider, sql_comparison, gpt-3.5-turbo\n",
        "\n",
        "import openai\n",
        "import json\n",
        "from google.colab import files\n",
        "# question_schemas = [\"What are the names of all the employees in the HR department?|CREATE TABLE Employees (id INT, name TEXT, department TEXT)\"]\n",
        "with open('serialized_dr_sql_comparison_data_str.json', 'r') as file:\n",
        "    question_schemas = json.load(file)\n",
        "\n",
        "instruction = \"You are a semantic parser to translate natural language question to SQL query. In each question, the database schema will be provided. In the SQL you don't need to indicate the name of the database, just write SQL within the database. Please give only the SQL queries without any text explanations\"\n",
        "example = \"Here is an example including input and output. Input: question: How many heads of the departments are older than 56 ? serialized_schema:  | department_management | department : department_id , name , creation , ranking , budget_in_billions , num_employees | head : head_id , name , born_state , age | management : department_id , head_id , temporary_acting. Output: select count(*) from head where age > 56\"\n",
        "example_2 = \"Here is another example including input and output. Input: question: List the name, born state and age of the heads of departments ordered by age. serialized_schema:  | department_management | department : department_id , name , creation , ranking , budget_in_billions , num_employees | head : head_id , name , born_state , age | management : department_id , head_id , temporary_acting. Output: select name, born_state, age from head order by age\"\n",
        "\n",
        "#messages = [{\"role\": \"system\", \"content\": instruction}]\n",
        "\n",
        "results,predictions = [],[]\n",
        "\n",
        "for question_schema in question_schemas: \n",
        "    #messages = [{\"role\": \"system\", \"content\": instruction}]\n",
        "    messages = [{\"role\": \"system\", \"content\": instruction},{\"role\": \"system\", \"content\": example}] \n",
        "    #messages = [{\"role\": \"system\", \"content\": instruction},{\"role\": \"system\", \"content\": example},{\"role\": \"system\", \"content\": example_2}] \n",
        "    messages.append({\"role\": \"user\", \"content\": question_schema})\n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        max_tokens=200,\n",
        "        temperature=0,\n",
        "        n = 1,\n",
        "        stop=None,\n",
        "        timeout=15,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "    chat_response= completion.choices[0].message.content\n",
        "    sql_query=chat_response.replace(\"\\n\", \" \")\n",
        "\n",
        "    print(sql_query)\n",
        "    messages.append({\"role\": \"assistant\", \"content\": chat_response}) \n",
        "    results.append({\"input\": question_schema, \"sql_query\": sql_query})\n",
        "    predictions.append(sql_query)\n",
        "\n",
        "# with open(\"results_gpt_3.5_turbo.json\", \"w\") as file:\n",
        "#     json.dump(results, file, indent=4)\n",
        "with open(\"predictions_gpt_3.5_turbo_sql_comparison.json\", \"w\") as file:\n",
        "    json.dump(predictions, file, indent=4)\n",
        "files.download(\"predictions_gpt_3.5_turbo_sql_comparison.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06TS_Bg15Wa-"
      },
      "outputs": [],
      "source": [
        "### Dr.Spider, sql_db_number, gpt-3.5-turbo\n",
        "\n",
        "import openai\n",
        "import json\n",
        "from google.colab import files\n",
        "# question_schemas = [\"What are the names of all the employees in the HR department?|CREATE TABLE Employees (id INT, name TEXT, department TEXT)\"]\n",
        "with open('serialized_dr_sql_db_number_data_str.json', 'r') as file:\n",
        "    question_schemas = json.load(file)\n",
        "\n",
        "instruction = \"You are a semantic parser to translate natural language question to SQL query. In each question, the database schema will be provided. In the SQL you don't need to indicate the name of the database, just write SQL within the database. Please give only the SQL queries without any text explanations\"\n",
        "example = \"Here is an example including input and output. Input: question: How many heads of the departments are older than 56 ? serialized_schema:  | department_management | department : department_id , name , creation , ranking , budget_in_billions , num_employees | head : head_id , name , born_state , age | management : department_id , head_id , temporary_acting. Output: select count(*) from head where age > 56\"\n",
        "example_2 = \"Here is another example including input and output. Input: question: List the name, born state and age of the heads of departments ordered by age. serialized_schema:  | department_management | department : department_id , name , creation , ranking , budget_in_billions , num_employees | head : head_id , name , born_state , age | management : department_id , head_id , temporary_acting. Output: select name, born_state, age from head order by age\"\n",
        "\n",
        "#messages = [{\"role\": \"system\", \"content\": instruction}]\n",
        "\n",
        "results,predictions = [],[]\n",
        "\n",
        "for question_schema in question_schemas: \n",
        "    #messages = [{\"role\": \"system\", \"content\": instruction}]\n",
        "    messages = [{\"role\": \"system\", \"content\": instruction},{\"role\": \"system\", \"content\": example}] \n",
        "    #messages = [{\"role\": \"system\", \"content\": instruction},{\"role\": \"system\", \"content\": example},{\"role\": \"system\", \"content\": example_2}] \n",
        "    messages.append({\"role\": \"user\", \"content\": question_schema})\n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        max_tokens=200,\n",
        "        temperature=0,\n",
        "        n = 1,\n",
        "        stop=None,\n",
        "        timeout=15,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "    chat_response= completion.choices[0].message.content\n",
        "    sql_query=chat_response.replace(\"\\n\", \" \")\n",
        "\n",
        "    print(sql_query)\n",
        "    messages.append({\"role\": \"assistant\", \"content\": chat_response}) \n",
        "    results.append({\"input\": question_schema, \"sql_query\": sql_query})\n",
        "    predictions.append(sql_query)\n",
        "\n",
        "# with open(\"results_gpt_3.5_turbo.json\", \"w\") as file:\n",
        "#     json.dump(results, file, indent=4)\n",
        "with open(\"predictions_gpt_3.5_turbo_sql_db_number.json\", \"w\") as file:\n",
        "    json.dump(predictions, file, indent=4)\n",
        "files.download(\"predictions_gpt_3.5_turbo_sql_db_number.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ciN72Wx_Mjl"
      },
      "outputs": [],
      "source": [
        "### Dr.Spider, sql_db_text, gpt-3.5-turbo\n",
        "\n",
        "import openai\n",
        "import json\n",
        "from google.colab import files\n",
        "# question_schemas = [\"What are the names of all the employees in the HR department?|CREATE TABLE Employees (id INT, name TEXT, department TEXT)\"]\n",
        "with open('serialized_dr_sql_db_text_data_str.json', 'r') as file:\n",
        "    question_schemas = json.load(file)\n",
        "\n",
        "instruction = \"You are a semantic parser to translate natural language question to SQL query. In each question, the database schema will be provided. In the SQL you don't need to indicate the name of the database, just write SQL within the database. Please give only the SQL queries without any text explanations\"\n",
        "example = \"Here is an example including input and output. Input: question: How many heads of the departments are older than 56 ? serialized_schema:  | department_management | department : department_id , name , creation , ranking , budget_in_billions , num_employees | head : head_id , name , born_state , age | management : department_id , head_id , temporary_acting. Output: select count(*) from head where age > 56\"\n",
        "example_2 = \"Here is another example including input and output. Input: question: List the name, born state and age of the heads of departments ordered by age. serialized_schema:  | department_management | department : department_id , name , creation , ranking , budget_in_billions , num_employees | head : head_id , name , born_state , age | management : department_id , head_id , temporary_acting. Output: select name, born_state, age from head order by age\"\n",
        "\n",
        "#messages = [{\"role\": \"system\", \"content\": instruction}]\n",
        "\n",
        "results,predictions = [],[]\n",
        "\n",
        "for question_schema in question_schemas: \n",
        "    #messages = [{\"role\": \"system\", \"content\": instruction}]\n",
        "    messages = [{\"role\": \"system\", \"content\": instruction},{\"role\": \"system\", \"content\": example}] \n",
        "    #messages = [{\"role\": \"system\", \"content\": instruction},{\"role\": \"system\", \"content\": example},{\"role\": \"system\", \"content\": example_2}] \n",
        "    messages.append({\"role\": \"user\", \"content\": question_schema})\n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        max_tokens=200,\n",
        "        temperature=0,\n",
        "        n = 1,\n",
        "        stop=None,\n",
        "        timeout=15,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "    chat_response= completion.choices[0].message.content\n",
        "    sql_query=chat_response.replace(\"\\n\", \" \")\n",
        "\n",
        "    print(sql_query)\n",
        "    messages.append({\"role\": \"assistant\", \"content\": chat_response}) \n",
        "    results.append({\"input\": question_schema, \"sql_query\": sql_query})\n",
        "    predictions.append(sql_query)\n",
        "\n",
        "# with open(\"results_gpt_3.5_turbo.json\", \"w\") as file:\n",
        "#     json.dump(results, file, indent=4)\n",
        "with open(\"predictions_gpt_3.5_turbo_sql_db_text.json\", \"w\") as file:\n",
        "    json.dump(predictions, file, indent=4)\n",
        "files.download(\"predictions_gpt_3.5_turbo_sql_db_text.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puyEt_dBEICP"
      },
      "outputs": [],
      "source": [
        "### Dr.Spider, sql_nondb_number, gpt-3.5-turbo\n",
        "\n",
        "import openai\n",
        "import json\n",
        "from google.colab import files\n",
        "# question_schemas = [\"What are the names of all the employees in the HR department?|CREATE TABLE Employees (id INT, name TEXT, department TEXT)\"]\n",
        "with open('serialized_dr_sql_nondb_number_data_str.json', 'r') as file:\n",
        "    question_schemas = json.load(file)\n",
        "\n",
        "instruction = \"You are a semantic parser to translate natural language question to SQL query. In each question, the database schema will be provided. In the SQL you don't need to indicate the name of the database, just write SQL within the database. Please give only the SQL queries without any text explanations\"\n",
        "example = \"Here is an example including input and output. Input: question: How many heads of the departments are older than 56 ? serialized_schema:  | department_management | department : department_id , name , creation , ranking , budget_in_billions , num_employees | head : head_id , name , born_state , age | management : department_id , head_id , temporary_acting. Output: select count(*) from head where age > 56\"\n",
        "example_2 = \"Here is another example including input and output. Input: question: List the name, born state and age of the heads of departments ordered by age. serialized_schema:  | department_management | department : department_id , name , creation , ranking , budget_in_billions , num_employees | head : head_id , name , born_state , age | management : department_id , head_id , temporary_acting. Output: select name, born_state, age from head order by age\"\n",
        "\n",
        "#messages = [{\"role\": \"system\", \"content\": instruction}]\n",
        "\n",
        "results,predictions = [],[]\n",
        "\n",
        "for question_schema in question_schemas: \n",
        "    #messages = [{\"role\": \"system\", \"content\": instruction}]\n",
        "    messages = [{\"role\": \"system\", \"content\": instruction},{\"role\": \"system\", \"content\": example}] \n",
        "    #messages = [{\"role\": \"system\", \"content\": instruction},{\"role\": \"system\", \"content\": example},{\"role\": \"system\", \"content\": example_2}] \n",
        "    messages.append({\"role\": \"user\", \"content\": question_schema})\n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        max_tokens=200,\n",
        "        temperature=0,\n",
        "        n = 1,\n",
        "        stop=None,\n",
        "        timeout=15,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "    chat_response= completion.choices[0].message.content\n",
        "    sql_query=chat_response.replace(\"\\n\", \" \")\n",
        "\n",
        "    print(sql_query)\n",
        "    messages.append({\"role\": \"assistant\", \"content\": chat_response}) \n",
        "    results.append({\"input\": question_schema, \"sql_query\": sql_query})\n",
        "    predictions.append(sql_query)\n",
        "\n",
        "# with open(\"results_gpt_3.5_turbo.json\", \"w\") as file:\n",
        "#     json.dump(results, file, indent=4)\n",
        "with open(\"predictions_gpt_3.5_turbo_sql_nondb_number.json\", \"w\") as file:\n",
        "    json.dump(predictions, file, indent=4)\n",
        "files.download(\"predictions_gpt_3.5_turbo_sql_nondb_number.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uk59ZtXLF9N9"
      },
      "outputs": [],
      "source": [
        "### Dr.Spider, sql_sort_order, gpt-3.5-turbo\n",
        "\n",
        "import openai\n",
        "import json\n",
        "from google.colab import files\n",
        "# question_schemas = [\"What are the names of all the employees in the HR department?|CREATE TABLE Employees (id INT, name TEXT, department TEXT)\"]\n",
        "with open('serialized_dr_sql_sort_order_data_str.json', 'r') as file:\n",
        "    question_schemas = json.load(file)\n",
        "\n",
        "instruction = \"You are a semantic parser to translate natural language question to SQL query. In each question, the database schema will be provided. In the SQL you don't need to indicate the name of the database, just write SQL within the database. Please give only the SQL queries without any text explanations\"\n",
        "example = \"Here is an example including input and output. Input: question: How many heads of the departments are older than 56 ? serialized_schema:  | department_management | department : department_id , name , creation , ranking , budget_in_billions , num_employees | head : head_id , name , born_state , age | management : department_id , head_id , temporary_acting. Output: select count(*) from head where age > 56\"\n",
        "example_2 = \"Here is another example including input and output. Input: question: List the name, born state and age of the heads of departments ordered by age. serialized_schema:  | department_management | department : department_id , name , creation , ranking , budget_in_billions , num_employees | head : head_id , name , born_state , age | management : department_id , head_id , temporary_acting. Output: select name, born_state, age from head order by age\"\n",
        "\n",
        "#messages = [{\"role\": \"system\", \"content\": instruction}]\n",
        "\n",
        "results,predictions = [],[]\n",
        "\n",
        "for question_schema in question_schemas: \n",
        "    #messages = [{\"role\": \"system\", \"content\": instruction}]\n",
        "    messages = [{\"role\": \"system\", \"content\": instruction},{\"role\": \"system\", \"content\": example}] \n",
        "    #messages = [{\"role\": \"system\", \"content\": instruction},{\"role\": \"system\", \"content\": example},{\"role\": \"system\", \"content\": example_2}] \n",
        "    messages.append({\"role\": \"user\", \"content\": question_schema})\n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        max_tokens=200,\n",
        "        temperature=0,\n",
        "        n = 1,\n",
        "        stop=None,\n",
        "        timeout=15,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "    chat_response= completion.choices[0].message.content\n",
        "    sql_query=chat_response.replace(\"\\n\", \" \")\n",
        "\n",
        "    print(sql_query)\n",
        "    messages.append({\"role\": \"assistant\", \"content\": chat_response}) \n",
        "    results.append({\"input\": question_schema, \"sql_query\": sql_query})\n",
        "    predictions.append(sql_query)\n",
        "\n",
        "# with open(\"results_gpt_3.5_turbo.json\", \"w\") as file:\n",
        "#     json.dump(results, file, indent=4)\n",
        "with open(\"predictions_gpt_3.5_turbo_sql_sort_order.json\", \"w\") as file:\n",
        "    json.dump(predictions, file, indent=4)\n",
        "files.download(\"predictions_gpt_3.5_turbo_sql_sort_order.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Rhfx0Ot5QKq",
        "outputId": "9f442b54-4e6c-45aa-cda7-1103e5050f85"
      },
      "outputs": [],
      "source": [
        "print(len(predictions))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_oOxffbJmP8J"
      },
      "source": [
        "## Generate adversarial examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwS4SIQQko_s"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import json\n",
        "import files\n",
        "openai.api_key = \"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ik66KsmWmTrl",
        "outputId": "4ae34991-b7cb-4240-f63f-856c14c2288d"
      },
      "outputs": [],
      "source": [
        "### An example: Char-level perturbation: creating typos\n",
        "\n",
        "instruction=\"You are to generate adversarial examples in text-to-SQL task by creating typos in the natural language question. In each question, the origin questionn and the database schema will be provided. You only need to output the post-perturbation question\"\n",
        "\n",
        "messages = [\n",
        " {\"role\": \"system\", \"content\" : instruction}\n",
        "]\n",
        "content = \"question: What is the total number of singers? serialized_schema:  | concert_singer | stadium : stadium_id , location , name , capacity , highest , lowest , average | singer : singer_id , name , country , song_name , song_release_year , age , is_male | concert : concert_id , concert_name , theme , stadium_id , year | singer_in_concert : concert_id , singer_id\"\n",
        "messages.append({\"role\": \"user\", \"content\": content})\n",
        "\n",
        "completion = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=messages\n",
        ")\n",
        "\n",
        "chat_response = completion.choices[0].message.content\n",
        "print(chat_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsEC9ZnLtFUs"
      },
      "outputs": [],
      "source": [
        "### Spider, Char-level perturbation: creating typos\n",
        "\n",
        "import openai\n",
        "import json\n",
        "from google.colab import files\n",
        "with open('serialized_data_str.json', 'r') as file:\n",
        "    question_schemas = json.load(file)\n",
        "\n",
        "instruction=\"You are to generate adversarial examples in text-to-SQL task by creating typos in the natural language question. In each question, the origin questionn and the database schema will be provided. You only need to output the post-perturbation question\"\n",
        "\n",
        "questions = []\n",
        "for question_schema in question_schemas: \n",
        "    messages = [{\"role\": \"system\", \"content\": instruction}]\n",
        "        \n",
        "    messages.append({\"role\": \"user\", \"content\": question_schema})\n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        max_tokens=200,\n",
        "        temperature=0.5,\n",
        "        n = 1,\n",
        "        stop=None,\n",
        "        timeout=15,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "    chat_response= completion.choices[0].message.content\n",
        "    post_perturbation=chat_response.replace(\"\\n\", \" \")\n",
        "    print(post_perturbation)\n",
        "    questions.append(post_perturbation)\n",
        "\n",
        "with open(\"perturbation_spider_typo.json\", \"w\") as file:\n",
        "    json.dump(questions, file, indent=4)\n",
        "files.download(\"perturbation_spider_typo.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5tUPJd7yJob"
      },
      "outputs": [],
      "source": [
        "### Spider-typos, gpt-3.5-turbo\n",
        "\n",
        "import openai\n",
        "import json\n",
        "from google.colab import files\n",
        "with open('serialized_typo_data_str.json', 'r') as file:\n",
        "    question_schemas = json.load(file)\n",
        "\n",
        "instruction = \"You are a semantic parser to translate natural language question to SQL query. In each question, the database schema will be provided. In the SQL you don't need to indicate the name of the database, just write SQL within the database. Please give only the SQL queries without any text explanations\"\n",
        "example = \"Here is an example including input and output. Input: question: How many heads of the departments are older than 56 ? serialized_schema:  | department_management | department : department_id , name , creation , ranking , budget_in_billions , num_employees | head : head_id , name , born_state , age | management : department_id , head_id , temporary_acting. Output: select count(*) from head where age > 56\"\n",
        "\n",
        "predictions = []\n",
        "\n",
        "for question_schema in question_schemas: \n",
        "    messages = [{\"role\": \"system\", \"content\": instruction},{\"role\": \"system\", \"content\": example}] \n",
        "    messages.append({\"role\": \"user\", \"content\": question_schema})\n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        max_tokens=200,\n",
        "        temperature=0,\n",
        "        n = 1,\n",
        "        stop=None,\n",
        "        timeout=15,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "    chat_response= completion.choices[0].message.content\n",
        "    sql_query=chat_response.replace(\"\\n\", \" \")\n",
        "\n",
        "    print(sql_query)\n",
        "    predictions.append(sql_query)\n",
        "\n",
        "with open(\"predictions_gpt_3.5_turbo_typo.json\", \"w\") as file:\n",
        "    json.dump(predictions, file, indent=4)\n",
        "files.download(\"predictions_gpt_3.5_turbo_typo.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhZQRPxamkko",
        "outputId": "4587df24-150e-4720-f0a5-64870cadcc29"
      },
      "outputs": [],
      "source": [
        "### An example: sentence-level perturbation: changing style\n",
        "\n",
        "instruction=\"You are to generate adversarial examples in text-to-SQL task by changing the writing style of the natural language question. e.g. changing imperative sentences to interrogative sentences In each question, the origin questionn and the database schema will be provided. You only need to output the post-perturbation question\"\n",
        "\n",
        "messages = [\n",
        " {\"role\": \"system\", \"content\" : instruction}\n",
        "]\n",
        "content = \"question: What is the total number of singers? serialized_schema:  | concert_singer | stadium : stadium_id , location , name , capacity , highest , lowest , average | singer : singer_id , name , country , song_name , song_release_year , age , is_male | concert : concert_id , concert_name , theme , stadium_id , year | singer_in_concert : concert_id , singer_id\"\n",
        "messages.append({\"role\": \"user\", \"content\": content})\n",
        "\n",
        "completion = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=messages,\n",
        "  temperature=1.5\n",
        ")\n",
        "\n",
        "chat_response = completion.choices[0].message.content\n",
        "print(chat_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FFonOwEDA27"
      },
      "outputs": [],
      "source": [
        "### Spider, Sentence level perturbation: changing styles\n",
        "\n",
        "import openai\n",
        "import json\n",
        "from google.colab import files\n",
        "with open('serialized_data_str.json', 'r') as file:\n",
        "    question_schemas = json.load(file)\n",
        "\n",
        "instruction=\"You are to generate adversarial examples in text-to-SQL task changing the writing style of the natural language question. In each question, the origin questionn and the database schema will be provided. You only need to output the post-perturbation question\"\n",
        "\n",
        "#questions = []\n",
        "for question_schema in question_schemas[552::]: \n",
        "    messages = [{\"role\": \"system\", \"content\": instruction}]\n",
        "        \n",
        "    messages.append({\"role\": \"user\", \"content\": question_schema})\n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        max_tokens=200,\n",
        "        temperature=0.5,\n",
        "        n = 1,\n",
        "        stop=None,\n",
        "        timeout=15,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "    chat_response= completion.choices[0].message.content\n",
        "    post_perturbation=chat_response.replace(\"\\n\", \" \")\n",
        "    print(post_perturbation)\n",
        "    questions.append(post_perturbation)\n",
        "\n",
        "with open(\"perturbation_spider_style.json\", \"w\") as file:\n",
        "    json.dump(questions, file, indent=4)\n",
        "files.download(\"perturbation_spider_style.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xJYNI7Z-dU7"
      },
      "outputs": [],
      "source": [
        "### Spider-style, gpt-3.5-turbo\n",
        "\n",
        "import openai\n",
        "import json\n",
        "from google.colab import files\n",
        "with open('serialized_style_data_str.json', 'r') as file:\n",
        "    question_schemas = json.load(file)\n",
        "\n",
        "instruction = \"You are a semantic parser to translate natural language question to SQL query. In each question, the database schema will be provided. In the SQL you don't need to indicate the name of the database, just write SQL within the database. Please give only the SQL queries without any text explanations\"\n",
        "example = \"Here is an example including input and output. Input: question: How many heads of the departments are older than 56 ? serialized_schema:  | department_management | department : department_id , name , creation , ranking , budget_in_billions , num_employees | head : head_id , name , born_state , age | management : department_id , head_id , temporary_acting. Output: select count(*) from head where age > 56\"\n",
        "\n",
        "predictions = []\n",
        "\n",
        "for question_schema in question_schemas: \n",
        "    messages = [{\"role\": \"system\", \"content\": instruction},{\"role\": \"system\", \"content\": example}] \n",
        "    messages.append({\"role\": \"user\", \"content\": question_schema})\n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        max_tokens=200,\n",
        "        temperature=0,\n",
        "        n = 1,\n",
        "        stop=None,\n",
        "        timeout=15,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "    chat_response= completion.choices[0].message.content\n",
        "    sql_query=chat_response.replace(\"\\n\", \" \")\n",
        "\n",
        "    print(sql_query)\n",
        "    predictions.append(sql_query)\n",
        "\n",
        "with open(\"predictions_gpt_3.5_turbo_style.json\", \"w\") as file:\n",
        "    json.dump(predictions, file, indent=4)\n",
        "files.download(\"predictions_gpt_3.5_turbo_style.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nvNcSjSwaTC",
        "outputId": "cc249d19-8b31-4fa7-881e-0a95bd1a8e91"
      },
      "outputs": [],
      "source": [
        "print(len(questions))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "acQDB18tKA36"
      },
      "source": [
        "## Temperature and prompt investigation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtvHNF-NJ67y"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import json\n",
        "from google.colab import files\n",
        "openai.api_key = \"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\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "c4JWN4p5MDGx"
      },
      "source": [
        "### temperature investigation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "3vglrG7IKIbd",
        "outputId": "93132909-853e-46dd-f030-ecb84ac93f6f"
      },
      "outputs": [],
      "source": [
        "### Spider, gpt3.5 temperature\n",
        "\n",
        "import openai\n",
        "import json\n",
        "from google.colab import files\n",
        "with open('serialized_data_str.json', 'r') as file:\n",
        "    question_schemas = json.load(file)\n",
        "\n",
        "instruction = \"You are a semantic parser to translate natural language question to SQL query. In each question, the database schema will be provided. In the SQL you don't need to indicate the name of the database, just write SQL within the database. Please give only the SQL queries without any text explanations\"\n",
        "example = \"Here is an example including input and output. Input: question: How many heads of the departments are older than 56 ? serialized_schema:  | department_management | department : department_id , name , creation , ranking , budget_in_billions , num_employees | head : head_id , name , born_state , age | management : department_id , head_id , temporary_acting. Output: select count(*) from head where age > 56\"\n",
        "predictions = []\n",
        "\n",
        "for question_schema in question_schemas[0:200]:     \n",
        "    messages = [{\"role\": \"system\", \"content\": instruction},{\"role\": \"system\", \"content\": example}] \n",
        "    messages.append({\"role\": \"user\", \"content\": question_schema})\n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        max_tokens=200,\n",
        "        temperature=2,\n",
        "        n = 1,\n",
        "        stop=None,\n",
        "        timeout=15,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "    chat_response= completion.choices[0].message.content\n",
        "    sql_query=chat_response.replace(\"\\n\", \" \")\n",
        "    #print(sql_query)\n",
        "    predictions.append(sql_query)\n",
        "\n",
        "with open(\"predictions_spider_temper_20.json\", \"w\") as file:\n",
        "    json.dump(predictions, file, indent=4)\n",
        "files.download(\"predictions_spider_temper_20.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "BwGsICXem9zn",
        "outputId": "1b9317a7-5d4a-4258-b3df-64de310b0f33"
      },
      "outputs": [],
      "source": [
        "## Spider, davinci temperature\n",
        "\n",
        "import openai\n",
        "import json\n",
        "from google.colab import files\n",
        "with open('serialized_data_str.json', 'r') as file:\n",
        "    question_schemas = json.load(file)\n",
        "\n",
        "instruction = \"You are a semantic parser to translate natural language question to SQL query. In each question, the database schema will be provided. In the SQL you don't need to indicate the name of the database, just write SQL within the database. Please give only the SQL queries without any text explanations\"\n",
        "example = \"Here is an example including input and output. Input: question: How many heads of the departments are older than 56 ? serialized_schema:  | department_management | department : department_id , name , creation , ranking , budget_in_billions , num_employees | head : head_id , name , born_state , age | management : department_id , head_id , temporary_acting. Output: select count(*) from head where age > 56\"\n",
        "predictions = []\n",
        "\n",
        "for question_schema in question_schemas[0:200]: \n",
        "    messages = [{\"role\": \"system\", \"content\": instruction}] \n",
        "    messages.append({\"role\": \"user\", \"content\": question_schema})\n",
        "    completion = openai.Completion.create(\n",
        "        engine=\"text-davinci-003\",\n",
        "        prompt=\"\".join([m[\"content\"] for m in messages]),\n",
        "        max_tokens=200,\n",
        "        temperature=2,\n",
        "        n = 1,\n",
        "        stop=None,\n",
        "        timeout=15,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "    chat_response = completion.choices[0].text.strip()\n",
        "    sql_query = chat_response.replace(\"\\n\", \" \")\n",
        "    #print(sql_query)\n",
        "    predictions.append(sql_query)\n",
        "\n",
        "\n",
        "with open(\"predictions_spider_davinci_temper_20.json\", \"w\") as file:\n",
        "    json.dump(predictions, file, indent=4)\n",
        "files.download(\"predictions_spider_davinci_temper_20.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AriZkoAtuCw9",
        "outputId": "f691731f-b1ef-4eab-a820-e330ae6a47f2"
      },
      "outputs": [],
      "source": [
        "print(len(predictions))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TZ-7deWmMHt3"
      },
      "source": [
        "### prompt investigation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FHoUjatkV0Y5"
      },
      "source": [
        "#### question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTYvlft1MKus"
      },
      "outputs": [],
      "source": [
        "### prompt, question\n",
        "\n",
        "import openai\n",
        "import json\n",
        "from google.colab import files\n",
        "with open('serialized_question.json', 'r') as file:\n",
        "    questions = json.load(file)\n",
        "\n",
        "instruction = \"You are a semantic parser to translate natural language question to SQL query. In the SQL you don't need to indicate the name of the database, just write SQL within the database. Please give only the SQL queries without any text explanations\"\n",
        "predictions = []\n",
        "\n",
        "for question in questions[0:200]:     \n",
        "    messages = [{\"role\": \"system\", \"content\": instruction}] \n",
        "    messages.append({\"role\": \"user\", \"content\": question})\n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        max_tokens=200,\n",
        "        temperature=0,\n",
        "        n = 1,\n",
        "        stop=None,\n",
        "        timeout=15,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "    chat_response= completion.choices[0].message.content\n",
        "    sql_query=chat_response.replace(\"\\n\", \" \")\n",
        "    print(sql_query)\n",
        "    predictions.append(sql_query)\n",
        "\n",
        "with open(\"predictions_spider_prompt_question.json\", \"w\") as file:\n",
        "    json.dump(predictions, file, indent=4)\n",
        "files.download(\"predictions_spider_prompt_question.json\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oQF1e8EFW0Kp"
      },
      "source": [
        "#### question schema (serialized format)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mi34X-Sr9SZ1"
      },
      "outputs": [],
      "source": [
        "### prompt question + schema (serialized format)\n",
        "\n",
        "import openai\n",
        "import json\n",
        "from google.colab import files\n",
        "with open('serialized_schema.json', 'r') as file:\n",
        "    questions = json.load(file)\n",
        "\n",
        "instruction = \"You are a semantic parser to translate natural language question to SQL query. In each question, the database schema will be provided. In the SQL you don't need to indicate the name of the database, just write SQL within the database. Please give only the SQL queries without any text explanations\"\n",
        "predictions = []\n",
        "\n",
        "for question in questions[0:200]:     \n",
        "    messages = [{\"role\": \"system\", \"content\": instruction}] \n",
        "    messages.append({\"role\": \"user\", \"content\": question})\n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        max_tokens=200,\n",
        "        temperature=0,\n",
        "        n = 1,\n",
        "        stop=None,\n",
        "        timeout=15,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "    chat_response= completion.choices[0].message.content\n",
        "    sql_query=chat_response.replace(\"\\n\", \" \")\n",
        "    print(sql_query)\n",
        "    predictions.append(sql_query)\n",
        "\n",
        "with open(\"predictions_spider_prompt_schema.json\", \"w\") as file:\n",
        "    json.dump(predictions, file, indent=4)\n",
        "files.download(\"predictions_spider_prompt_schema.json\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zF1QCpgFX6a1"
      },
      "source": [
        "#### question schema (API doc format)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "to781ybIX_Gd"
      },
      "outputs": [],
      "source": [
        "### prompt question + schema (API doc format)\n",
        "\n",
        "import openai\n",
        "import json\n",
        "from google.colab import files\n",
        "with open('serialized_api_doc.json', 'r') as file:\n",
        "    questions = json.load(file)\n",
        "\n",
        "predictions = []\n",
        "\n",
        "for question in questions[0:200]:     \n",
        "    messages = [] \n",
        "    messages.append({\"role\": \"user\", \"content\": question})\n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        max_tokens=200,\n",
        "        temperature=0,\n",
        "        n = 1,\n",
        "        stop=None,\n",
        "        timeout=15,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "    chat_response= completion.choices[0].message.content\n",
        "    sql_query=chat_response.replace(\"\\n\", \" \")\n",
        "    sql_query= \"SELECT \"+sql_query\n",
        "    print(sql_query)\n",
        "    predictions.append(sql_query)\n",
        "\n",
        "with open(\"predictions_spider_prompt_apidoc.json\", \"w\") as file:\n",
        "    json.dump(predictions, file, indent=4)\n",
        "files.download(\"predictions_spider_prompt_apidoc.json\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KdQaXHALX_mT"
      },
      "source": [
        "#### question create table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJ_k6qd0YCjt"
      },
      "outputs": [],
      "source": [
        "### prompt question + create table\n",
        "\n",
        "import openai\n",
        "import json\n",
        "from google.colab import files\n",
        "with open('serialized_create_command.json', 'r') as file:\n",
        "    questions = json.load(file)\n",
        "\n",
        "instruction = \"You are a semantic parser to translate natural language question to SQL query. In each question, the CREATE TABLE command of the database will be provided. In the SQL you don't need to indicate the name of the database, just write SQL within the database. Please give only the SQL queries without any text explanations\"\n",
        "predictions = []\n",
        "\n",
        "for question in questions[0:200]:     \n",
        "    messages = [{\"role\": \"system\", \"content\": instruction}] \n",
        "    messages.append({\"role\": \"user\", \"content\": question})\n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        max_tokens=200,\n",
        "        temperature=0,\n",
        "        n = 1,\n",
        "        stop=None,\n",
        "        timeout=15,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "    chat_response= completion.choices[0].message.content\n",
        "    sql_query=chat_response.replace(\"\\n\", \" \")\n",
        "    print(sql_query)\n",
        "    predictions.append(sql_query)\n",
        "\n",
        "with open(\"predictions_spider_prompt_create_table.json\", \"w\") as file:\n",
        "    json.dump(predictions, file, indent=4)\n",
        "files.download(\"predictions_spider_prompt_create_table.json\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CQNJhjzCYC3_"
      },
      "source": [
        "#### one shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NLY6WvbYHpb"
      },
      "outputs": [],
      "source": [
        "### prompt question + schema + example (one shot)\n",
        "\n",
        "import openai\n",
        "import json\n",
        "from google.colab import files\n",
        "with open('serialized_one_shot.json', 'r') as file:\n",
        "    questions = json.load(file)\n",
        "\n",
        "instruction = \"You are a semantic parser to translate natural language question to SQL query. In each question, the database schema will be provided. An example will also be provided to show the input and output format. In the SQL you don't need to indicate the name of the database, just write SQL within the database. Please give only the SQL queries without any text explanations\"\n",
        "predictions = []\n",
        "\n",
        "for question in questions[0:200]:     \n",
        "    messages = [{\"role\": \"system\", \"content\": instruction}] \n",
        "    messages.append({\"role\": \"user\", \"content\": question})\n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        max_tokens=200,\n",
        "        temperature=0,\n",
        "        n = 1,\n",
        "        stop=None,\n",
        "        timeout=15,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "    chat_response= completion.choices[0].message.content\n",
        "    sql_query=chat_response.replace(\"\\n\", \" \")\n",
        "    print(sql_query)\n",
        "    predictions.append(sql_query)\n",
        "\n",
        "with open(\"predictions_spider_prompt_one_shot.json\", \"w\") as file:\n",
        "    json.dump(predictions, file, indent=4)\n",
        "files.download(\"predictions_spider_prompt_one_shot.json\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0KhhZ-HYxY5"
      },
      "source": [
        "#### 5 shots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JF_gvUcHYzeX"
      },
      "outputs": [],
      "source": [
        "### prompt question + schema + examples (5 shots)\n",
        "\n",
        "import openai\n",
        "import json\n",
        "from google.colab import files\n",
        "with open('serialized_5shot.json', 'r') as file:\n",
        "    questions = json.load(file)\n",
        "\n",
        "instruction = \"You are a semantic parser to translate natural language question to SQL query. In each question, the CREATE TABLE command of the database will be provided. In the SQL you don't need to indicate the name of the database, just write SQL within the database. Please give only the SQL queries without any text explanations\"\n",
        "predictions = []\n",
        "\n",
        "for question in questions[0:200]:     \n",
        "    messages = [{\"role\": \"system\", \"content\": instruction}] \n",
        "    messages.append({\"role\": \"user\", \"content\": question})\n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        max_tokens=200,\n",
        "        temperature=0,\n",
        "        n = 1,\n",
        "        stop=None,\n",
        "        timeout=15,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "    chat_response= completion.choices[0].message.content\n",
        "    sql_query=chat_response.replace(\"\\n\", \" \")\n",
        "    print(sql_query)\n",
        "    predictions.append(sql_query)\n",
        "\n",
        "with open(\"predictions_spider_prompt_5shot.json\", \"w\") as file:\n",
        "    json.dump(predictions, file, indent=4)\n",
        "files.download(\"predictions_spider_prompt_5shot.json\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "CGFw0z-3ehhH",
        "OrnkXVb1BBq4",
        "ncCVu1syArrR",
        "juw50ca5AxEj",
        "2NQ2Mp7Pebpf",
        "PC2_SR3itp9_",
        "BJM8zWms4gsw",
        "EkkFQtxuSePD",
        "mH2FnFzj0yKw",
        "H_AaRLX26qBD",
        "6ppNyciI3lX5",
        "_oOxffbJmP8J",
        "FHoUjatkV0Y5",
        "oQF1e8EFW0Kp",
        "KdQaXHALX_mT",
        "CQNJhjzCYC3_",
        "Z0KhhZ-HYxY5"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
